{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e51560f-ee69-483f-8efe-636941e0a68e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FCDLツール（横浜ダイアログ用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4d815c-ed9b-4cad-b210-fca9b769a4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     ---------------------------------------- 0.0/86.0 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 30.7/86.0 kB 640.0 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 86.0/86.0 kB 1.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.32.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Collecting torch>=1.6.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for torch>=1.6.0 from https://files.pythonhosted.org/packages/e4/ae/2ad8820045b6631965750435f28583e80905b8273d57cf026163b51323ee/torch-2.1.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torch-2.1.2-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting torchvision (from sentence-transformers)\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/f9/e6/3c821e7417acd82df89e39f09156ce80d58817b5b4b1ac5453b522bc5dd4/torchvision-0.16.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torchvision-0.16.2-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Collecting sentencepiece (from sentence-transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 977.5/977.5 kB 20.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.15.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Downloading torch-2.1.2-cp311-cp311-win_amd64.whl (192.3 MB)\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.5/192.3 MB 52.1 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 5.4/192.3 MB 57.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 8.6/192.3 MB 60.9 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 11.3/192.3 MB 65.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 14.6/192.3 MB 65.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 18.6/192.3 MB 65.6 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 21.6/192.3 MB 73.1 MB/s eta 0:00:03\n",
      "   ----- --------------------------------- 26.1/192.3 MB 110.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 30.6/192.3 MB 93.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 35.4/192.3 MB 93.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 40.1/192.3 MB 93.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 45.1/192.3 MB 93.0 MB/s eta 0:00:02\n",
      "   --------- ----------------------------- 48.1/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   ---------- ---------------------------- 54.0/192.3 MB 110.0 MB/s eta 0:00:02\n",
      "   ----------- --------------------------- 58.8/192.3 MB 131.2 MB/s eta 0:00:02\n",
      "   ------------ -------------------------- 63.6/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 67.3/192.3 MB 93.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------ 72.3/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 76.6/192.3 MB 93.0 MB/s eta 0:00:02\n",
      "   ---------------- ---------------------- 81.6/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "   ----------------- --------------------- 84.1/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 86.6/192.3 MB 73.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 90.2/192.3 MB 65.6 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 95.8/192.3 MB 93.9 MB/s eta 0:00:02\n",
      "   ------------------- ------------------ 100.2/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------- ----------------- 105.2/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------- ---------------- 108.7/192.3 MB 93.0 MB/s eta 0:00:01\n",
      "   ---------------------- ---------------- 113.4/192.3 MB 93.0 MB/s eta 0:00:01\n",
      "   ----------------------- --------------- 118.0/192.3 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------------ -------------- 121.6/192.3 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------------- ------------ 127.1/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------------- ----------- 131.8/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 136.3/192.3 MB 93.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 140.6/192.3 MB 93.0 MB/s eta 0:00:01\n",
      "   ---------------------------- --------- 146.0/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 150.8/192.3 MB 93.0 MB/s eta 0:00:01\n",
      "   ------------------------------ ------- 156.0/192.3 MB 110.0 MB/s eta 0:00:01\n",
      "   ------------------------------- ------ 160.1/192.3 MB 110.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ----- 165.4/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ---- 170.3/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 174.3/192.3 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 179.4/192.3 MB 93.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ - 184.1/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------------------------  189.2/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------------------------  192.3/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------------------------  192.3/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------------------------  192.3/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------------------------  192.3/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------------------------  192.3/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------------------------  192.3/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   -------------------------------------  192.3/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 192.3/192.3 MB 21.1 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.16.2-cp311-cp311-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 70.5 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125960 sha256=d4906a56db6a542500f4f9927504f523c8240dac25ea4b36556fa6c365aca1a4\n",
      "  Stored in directory: c:\\users\\xding\\appdata\\local\\pip\\cache\\wheels\\ff\\27\\bf\\ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentencepiece, torch, torchvision, sentence-transformers\n",
      "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99 torch-2.1.2 torchvision-0.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\xding\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai==0.28\n",
      "  Obtaining dependency information for openai==0.28 from https://files.pythonhosted.org/packages/ae/59/911d6e5f1d7514d79c527067643376cddcf4cb8d1728e599b3b03ab51c69/openai-0.28.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from openai==0.28) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from openai==0.28) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->openai==0.28) (0.4.6)\n",
      "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.5 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 41.0/76.5 kB 991.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 76.5/76.5 kB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: openai\n",
      "Successfully installed openai-0.28.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script openai.exe is installed in 'C:\\Users\\xding\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyvis\n",
      "  Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
      "     ---------------------------------------- 0.0/756.0 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/756.0 kB ? eta -:--:--\n",
      "     --- --------------------------------- 71.7/756.0 kB 777.7 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 624.6/756.0 kB 4.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 756.0/756.0 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyvis) (8.15.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyvis) (3.1.2)\n",
      "Collecting jsonpickle>=1.4.1 (from pyvis)\n",
      "  Obtaining dependency information for jsonpickle>=1.4.1 from https://files.pythonhosted.org/packages/d3/25/6e0a450430b7aa194b0f515f64820fc619314faa289458b7dfca4a026114/jsonpickle-3.0.2-py3-none-any.whl.metadata\n",
      "  Downloading jsonpickle-3.0.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: networkx>=1.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyvis) (3.1)\n",
      "Requirement already satisfied: backcall in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2>=2.9.6->pyvis) (2.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from asttokens->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n",
      "Downloading jsonpickle-3.0.2-py3-none-any.whl (40 kB)\n",
      "   ---------------------------------------- 0.0/40.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 40.7/40.7 kB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: jsonpickle, pyvis\n",
      "Successfully installed jsonpickle-3.0.2 pyvis-0.3.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyathena\n",
      "  Obtaining dependency information for pyathena from https://files.pythonhosted.org/packages/84/36/6777e899043845ec923dc3b946587ad6599199ab5217a422b4484b4a1080/pyathena-3.0.10-py3-none-any.whl.metadata\n",
      "  Downloading pyathena-3.0.10-py3-none-any.whl.metadata (72 kB)\n",
      "     ---------------------------------------- 0.0/73.0 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 10.2/73.0 kB ? eta -:--:--\n",
      "     -------------------------------------  71.7/73.0 kB 975.2 kB/s eta 0:00:01\n",
      "     -------------------------------------- 73.0/73.0 kB 804.2 kB/s eta 0:00:00\n",
      "Collecting boto3>=1.26.4 (from pyathena)\n",
      "  Obtaining dependency information for boto3>=1.26.4 from https://files.pythonhosted.org/packages/e4/33/020d3f80c26a0c3fe086c3bb594c1767c09fc6207fce6c287cb8d4fc121a/boto3-1.34.3-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.34.3-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: botocore>=1.29.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyathena) (1.29.76)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from pyathena) (2023.4.0)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyathena) (8.2.2)\n",
      "Collecting botocore>=1.29.4 (from pyathena)\n",
      "  Obtaining dependency information for botocore>=1.29.4 from https://files.pythonhosted.org/packages/c8/6f/2079cda0cf55175318023433822090ccb8e7a65b838e7dbd47863544f15c/botocore-1.34.3-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.34.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3>=1.26.4->pyathena) (0.10.0)\n",
      "Collecting s3transfer<0.10.0,>=0.9.0 (from boto3>=1.26.4->pyathena)\n",
      "  Obtaining dependency information for s3transfer<0.10.0,>=0.9.0 from https://files.pythonhosted.org/packages/fd/fb/46eda754e80fa2efd82981e37cd75cabbecef71df63843e6e94e12fae9db/s3transfer-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading s3transfer-0.9.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore>=1.29.4->pyathena) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore>=1.29.4->pyathena) (1.26.16)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.29.4->pyathena) (1.16.0)\n",
      "Downloading pyathena-3.0.10-py3-none-any.whl (73 kB)\n",
      "   ---------------------------------------- 0.0/73.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 73.5/73.5 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading boto3-1.34.3-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.3/139.3 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading botocore-1.34.3-py3-none-any.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/11.8 MB 12.5 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.9/11.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.6/11.8 MB 12.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.1/11.8 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.1/11.8 MB 13.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.1/11.8 MB 16.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.9/11.8 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.7/11.8 MB 17.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.4/11.8 MB 17.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.6/11.8 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.6/11.8 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/11.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 25.2 MB/s eta 0:00:00\n",
      "Downloading s3transfer-0.9.0-py3-none-any.whl (82 kB)\n",
      "   ---------------------------------------- 0.0/82.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 82.0/82.0 kB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: botocore, s3transfer, boto3, pyathena\n",
      "Successfully installed boto3-1.34.3 botocore-1.34.3 pyathena-3.0.10 s3transfer-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.5.0 requires botocore<1.29.77,>=1.29.76, but you have botocore 1.34.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-api-python-client\n",
      "  Obtaining dependency information for google-api-python-client from https://files.pythonhosted.org/packages/62/1b/3ff6bd5f33c1a1780835725014ac480128d2d1e3244b2809275d0fa4f726/google_api_python_client-2.111.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_api_python_client-2.111.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 0.0/96.9 kB ? eta -:--:--\n",
      "     ------------ --------------------------- 30.7/96.9 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 96.9/96.9 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting google-auth<3.0.0.dev0,>=1.19.0 (from google-api-python-client)\n",
      "  Obtaining dependency information for google-auth<3.0.0.dev0,>=1.19.0 from https://files.pythonhosted.org/packages/f4/d2/9f6f3b9c0fd486617816cff42e856afea079d0bad99f0e60dc186c76b881/google_auth-2.25.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.25.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0 (from google-api-python-client)\n",
      "  Obtaining dependency information for google-auth-httplib2>=0.1.0 from https://files.pythonhosted.org/packages/be/8a/fe34d2f3f9470a27b01c9e76226965863f153d5fbe276f83608562e49c04/google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Obtaining dependency information for google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 from https://files.pythonhosted.org/packages/d6/c9/0462f037b62796fbda4801be62d0ae3147eaeb99e2939661580c98abe3eb/google_api_core-2.15.0-py3-none-any.whl.metadata\n",
      "  Downloading google_api_core-2.15.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Obtaining dependency information for googleapis-common-protos<2.0.dev0,>=1.56.2 from https://files.pythonhosted.org/packages/f0/43/c9d8f75ddf08e2a0a27db243c13a700c3cc7ec615b545b697cf6f715ad92/googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Obtaining dependency information for protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 from https://files.pythonhosted.org/packages/fe/6b/7f177e8d6fe4caa14f4065433af9f879d4fab84f0d17dcba7b407f6bd808/protobuf-4.25.1-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.1-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.7.22)\n",
      "Downloading google_api_python_client-2.111.0-py2.py3-none-any.whl (13.0 MB)\n",
      "   ---------------------------------------- 0.0/13.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/13.0 MB 34.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.1/13.0 MB 43.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.6/13.0 MB 54.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.2/13.0 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.0/13.0 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.0/13.0 MB 59.5 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.15.0-py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 0.0/122.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.0/122.0 kB ? eta 0:00:00\n",
      "Downloading google_auth-2.25.2-py2.py3-none-any.whl (184 kB)\n",
      "   ---------------------------------------- 0.0/184.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 184.2/184.2 kB 10.9 MB/s eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "   ---------------------------------------- 0.0/228.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 228.7/228.7 kB 13.7 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.1-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 413.4/413.4 kB ? eta 0:00:00\n",
      "Installing collected packages: uritemplate, rsa, protobuf, httplib2, cachetools, googleapis-common-protos, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed cachetools-5.3.2 google-api-core-2.15.0 google-api-python-client-2.111.0 google-auth-2.25.2 google-auth-httplib2-0.2.0 googleapis-common-protos-1.62.0 httplib2-0.22.0 protobuf-4.25.1 rsa-4.9 uritemplate-4.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\xding\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "### Terminalで: sudo yum install -y ipa-gothic-fonts\n",
    "!pip install sentence-transformers\n",
    "!pip install openai==0.28\n",
    "!pip install pyvis\n",
    "!pip install pyathena\n",
    "!pip install --upgrade google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ab7965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import re\n",
    "from scipy import spatial\n",
    "from pyvis.network import Network\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import openai\n",
    "import numpy as np\n",
    "import pyathena\n",
    "import pickle\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.cluster import KMeans\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# APIキーの設定\n",
    "apis = pd.read_csv(\"s3://lnext-for-quicksight/keys/apis.csv\")\n",
    "openai.api_key = apis[\"openai\"][0]\n",
    "API_KEY = apis[\"googlesearch\"][0]\n",
    "CUSTOM_SEARCH_ENGINE = 'a78a2783d10a44be6'\n",
    "page_limit = 1\n",
    "PROMPT_TEXT_KADAI_CORP = '''\n",
    "    この文章の主題を15文字以上30文字未満で要約してください。\n",
    "'''\n",
    "\n",
    "PROMPT_TEXT_FC = '''\n",
    "    この文章の主題を要約すると次の5個です。それぞれ15文字以上30文字未満で要約してください。\n",
    "'''\n",
    "\n",
    "PROMPT_TEXT_OTR = '''\n",
    "    この文章の主題を要約すると次の3個です。それぞれ15文字以上30文字未満で要約してください。\n",
    "'''\n",
    "\n",
    "# Sentence transformersのモデルの読み込み\n",
    "model = SentenceTransformer('stsb-xlm-r-multilingual')\n",
    "\n",
    "# アウトプットのトークン数。長い文章を返す場合には大きな数字にする。\n",
    "OUTPUT_TOKENS = 1000\n",
    "\n",
    "# distanceの数式の変更\n",
    "MAX_DISTANCE = 3.0\n",
    "EMPTY_TEXT = ''\n",
    "SIMILARITY_UPPER_THRESHOLD = 3.00\n",
    "SIMILARITY_LOWER_THRESHOLD = 2.65\n",
    "RESCALED_MAX_SIMILARITY_VALUE = 3.0\n",
    "RESCALED_MIN_SIMILARITY_VALUE = 0.1\n",
    "\n",
    "distance_fc_list = []\n",
    "distance_otr_list = []\n",
    "distance_fc_and_otr_list = []\n",
    "\n",
    "def getImageUrl(api_key, cse_key, search_word):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    page_limit = 1\n",
    "    startIndex = 1\n",
    "    response = []\n",
    "    img_list = []\n",
    "\n",
    "    try:\n",
    "        response.append(service.cse().list(\n",
    "            q=search_word,     # Search words\n",
    "            cx=cse_key,        # custom search engine key\n",
    "            lr='lang_ja',      # Search language\n",
    "            num=1,            # Number of images obtained by one request (Max 10)\n",
    "            start=startIndex,\n",
    "            searchType='image' # search for images\n",
    "        ).execute())\n",
    "\n",
    "        startIndex = response[0].get(\"queries\").get(\"nextPage\")[0].get(\"startIndex\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    for one_res in range(len(response)):\n",
    "        if int(response[one_res][\"searchInformation\"][\"totalResults\"]) > 0:\n",
    "            for i in range(len(response[one_res]['items'])):\n",
    "                img_list.append(response[one_res]['items'][i]['link'])\n",
    "        else:\n",
    "            img_list.append(\"https://1.bp.blogspot.com/-d3vDLBoPktU/WvQHWMBRhII/AAAAAAABL6E/Grg-XGzr9jEODAxkRcbqIXu-mFA9gTp3wCLcBGAs/s800/internet_404_page_not_found.png\")\n",
    "\n",
    "    return img_list[0]\n",
    "\n",
    "# 結果を取得する関数の定義\n",
    "def get_results_for_one_text(in_text, prompt):\n",
    "    response = openai.Completion.create(\n",
    "        # model=\"gpt-4\",\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=_get_cleaned_text(in_text) + prompt,\n",
    "        temperature=0, #temperature=0.5,\n",
    "        max_tokens=OUTPUT_TOKENS,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.8,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "    return _get_numbering_removed_keyword(\n",
    "        _get_parsed_result_by_return(\n",
    "            response[\"choices\"][0][\"text\"]\n",
    "        ))\n",
    "\n",
    "\n",
    "# インプットのテキストを整形する関数の定義。全角スペースを半角スペースに変換。\n",
    "def _get_cleaned_text(in_text):\n",
    "    in_text = in_text.replace('　', ' ')\n",
    "    return in_text\n",
    "\n",
    "def _get_empty_text_removed_list(in_list):\n",
    "    return [s for s in in_list if s != EMPTY_TEXT]\n",
    "\n",
    "def _get_parsed_result_by_return(in_sentence):\n",
    "    return _get_empty_text_removed_list(in_sentence.split('\\n'))\n",
    "\n",
    "def _get_numbering_removed_keyword(in_keyword_list):\n",
    "    this_result = []\n",
    "    for this_keyword in in_keyword_list:\n",
    "        this_keyword = re.sub(r'^\\d+\\. ', '', this_keyword)\n",
    "        this_result.append(re.sub(r'^\\d+\\.', '', this_keyword))\n",
    "    return this_result\n",
    "\n",
    "# 文章ベクトルを取得する関数の定義\n",
    "def get_sentence_vector(in_sentence):\n",
    "    return model.encode(in_sentence, convert_to_tensor=False)\n",
    "\n",
    "# 二つの文章ベクトルの間の距離を取得する関数の定義\n",
    "def get_similarity(in_vec_a, in_vec_b):\n",
    "    return MAX_DISTANCE - spatial.distance.cosine(in_vec_a, in_vec_b)\n",
    "\n",
    "def create_fcdl_network(fc_df, key_pair_df, image_flag, fname, tp, emb_df, network_num, orig_df):\n",
    "    flag_master = pd.DataFrame({\n",
    "        \"User\": [\"FC\", \"DLData\", \"DLKadai\", \"DLCorp\", \"DLHojo\", \"UserAns\"], \n",
    "        \"Flag\": [1, 2, 3, 4, 5, 6], \n",
    "        \"NodeColor\": [\"#66c2a5\", \"#fc8d62\", \"#8da0cb\", \"#e78ac3\", \"#a6d854\", \"#ffd92f\"], \n",
    "        \"EdgeColor\": [\"#b3e2cd\", \"#fdcdac\", \"#cbd5e8\", \"#f4cae4\", \"#e6f5c9\", \"#fff2ae\"]\n",
    "    })\n",
    "    \n",
    "    connect_df1 = fc_df[[\"Keyword\", \"UserCluster\"]]\n",
    "    connect_df1.columns = [\"key1\", \"key2\"]\n",
    "    connect_df2 = key_pair_df.sort_values([\"similarity\"], ascending=False).head(network_num).reset_index(drop=True)[[\"key1\", \"key2\"]]\n",
    "    \n",
    "    if tp == \"fcdl\":\n",
    "        fcdl_k1 = []\n",
    "        fcdl_k2 = []\n",
    "        for dttp in [\"DLData\", \"DLKadai\", \"DLCorp\", \"DLHojo\", \"UserAns\"]:\n",
    "            for i in range(len(emb_df)):\n",
    "                fcdl_k1.append(f\"FC{str(i)}\")\n",
    "                fcdl_k2.append(f\"{dttp}{str(i)}\")\n",
    "        connect_df3 = pd.DataFrame({\n",
    "            'key1': fcdl_k1, \n",
    "            'key2': fcdl_k2\n",
    "        })\n",
    "        \n",
    "        connect_df4 = fc_df[[\"Keyword\", \"Corp\"]].copy()\n",
    "        connect_df4 = connect_df4[connect_df4[\"Keyword\"] != \"\"]\n",
    "        connect_df4 = connect_df4[connect_df4[\"Corp\"] != \"\"]\n",
    "        connect_df4.columns = [\"key1\", \"key2\"]\n",
    "        connect_df5 = orig_df[[\"Keyword\", \"User\"]].copy()\n",
    "        connect_df5 = connect_df5[connect_df5[\"Keyword\"] != \"\"]\n",
    "        connect_df5 = connect_df5[connect_df5[\"User\"] != \"\"]\n",
    "        connect_df5.columns = [\"key1\", \"key2\"]\n",
    "        # connect_df6 = orig_df[orig_df[\"Answer\"] == 1].reset_index(drop=True).copy()\n",
    "        # connect_df6 = orig_df.copy()\n",
    "        # connect_df6 = connect_df6[[\"Keyword\", \"Cluster\"]].copy()\n",
    "        # connect_df6[\"Cluster\"] = [f\"FC{str(clst)}\" for clst in connect_df6[\"Cluster\"]]\n",
    "        # connect_df6 = connect_df6[connect_df6[\"Keyword\"] != \"\"]\n",
    "        # connect_df6 = connect_df6[connect_df6[\"Cluster\"] != \"\"]\n",
    "        # connect_df6.columns = [\"key1\", \"key2\"]\n",
    "        \n",
    "        \n",
    "        connect_df = pd.concat([connect_df1, connect_df2, connect_df3, connect_df4, connect_df5]).dropna().drop_duplicates().reset_index(drop=True)\n",
    "    else:\n",
    "        connect_df = pd.concat([connect_df1, connect_df2]).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    onlyfc_gp = pd.merge(fc_df[[\"Keyword\", \"User\"]], flag_master[[\"User\", \"Flag\"]], on=\"User\", how=\"left\")[[\"Keyword\", \"Flag\"]]\n",
    "    onlyfc_gp.columns = [\"NodeName\", \"Flag\"]\n",
    "    \n",
    "    node_df1 = connect_df[[\"key1\"]]\n",
    "    node_df1.columns = [\"NodeName\"]\n",
    "    node_df2 = connect_df[[\"key2\"]]\n",
    "    node_df2.columns = [\"NodeName\"]\n",
    "    node_df = pd.concat([node_df1, node_df2]).drop_duplicates().reset_index(drop=True)\n",
    "    node_df[\"NodeNum\"] = [1+i for i in range(len(node_df))]\n",
    "    node_df = pd.merge(node_df, onlyfc_gp, on=\"NodeName\", how=\"left\")\n",
    "    node_df[\"Flag\"] = node_df[\"Flag\"].fillna(0)\n",
    "\n",
    "    if image_flag:\n",
    "        img_list = []\n",
    "        for nd_i in range(len(node_df)):\n",
    "            search_word = node_df[\"NodeName\"][nd_i]\n",
    "            print(search_word)\n",
    "            img = getImageUrl(API_KEY, CUSTOM_SEARCH_ENGINE, search_word)\n",
    "            img_list.append(img)\n",
    "        node_df['ImageURL'] = img_list\n",
    "    \n",
    "    connect_df = pd.merge(connect_df, node_df, left_on=\"key1\", right_on=\"NodeName\", how=\"left\")[[\"key1\", \"key2\", \"NodeNum\", \"Flag\"]]\n",
    "    connect_df.columns = [\"key1\", \"key2\", \"NodeNum1\", \"Flag1\"]\n",
    "    connect_df = pd.merge(connect_df, node_df, left_on=\"key2\", right_on=\"NodeName\", how=\"left\")[[\"key1\", \"key2\", \"NodeNum1\", \"Flag1\", \"NodeNum\", \"Flag\"]]\n",
    "    connect_df.columns = [\"key1\", \"key2\", \"NodeNum1\", \"Flag1\", \"NodeNum2\", \"Flag2\"]\n",
    "    # connect_df[\"Flag1\"] = [1 if k1[:2] == \"FC\" else flg for flg, k1 in zip(connect_df[\"Flag1\"], connect_df[\"key1\"])]\n",
    "    # connect_df[\"Flag2\"] = [1 if k2[:2] == \"FC\" else flg for flg, k2 in zip(connect_df[\"Flag2\"], connect_df[\"key2\"])]\n",
    "    connect_df = connect_df.fillna(0)\n",
    "    connect_df[\"FCConnect\"] = connect_df[\"Flag1\"] * connect_df[\"Flag2\"]\n",
    "\n",
    "    # ネットワークのインスタンス生成\n",
    "    network = Network(\n",
    "        height=\"1000px\",  # デフォルト \"500px\"\n",
    "        width=\"2000px\",  # デフォルト \"500px\"\n",
    "        notebook=True,  # これをTrueにしておくとjupyter上で結果が見れる\n",
    "        bgcolor='#ffffff',  # 背景色。デフォルト \"#ffffff\"\n",
    "        directed=False,  # Trueにすると有向グラフ。デフォルトはFalseで無向グラフ\n",
    "    )\n",
    "\n",
    "    # add_node でノードを追加\n",
    "    for i in range(len(node_df)):\n",
    "        nd1_id = int(node_df['NodeNum'][i])\n",
    "        nd1_name = node_df['NodeName'][i]\n",
    "        nd1_flag = node_df['Flag'][i]\n",
    "\n",
    "        if nd1_flag == 1:\n",
    "            nd1_color = flag_master[\"EdgeColor\"][0]\n",
    "        elif nd1_flag == 2:\n",
    "            nd1_color = flag_master[\"EdgeColor\"][1]\n",
    "        elif nd1_flag == 3:\n",
    "            nd1_color = flag_master[\"EdgeColor\"][2]\n",
    "        elif nd1_flag == 4:\n",
    "            nd1_color = flag_master[\"EdgeColor\"][3]\n",
    "        elif nd1_flag == 5:\n",
    "            nd1_color = flag_master[\"EdgeColor\"][4]\n",
    "        elif nd1_flag == 6:\n",
    "            nd1_color = flag_master[\"EdgeColor\"][5]\n",
    "        elif nd1_name[:2] == \"FC\":\n",
    "            nd1_color = flag_master[\"NodeColor\"][0]\n",
    "        elif nd1_name[:6] == \"DLData\":\n",
    "            nd1_color = flag_master[\"NodeColor\"][1]\n",
    "        elif nd1_name[:7] == \"DLKadai\":\n",
    "            nd1_color = flag_master[\"NodeColor\"][2]\n",
    "        elif nd1_name[:6] == \"DLCorp\":\n",
    "            nd1_color = flag_master[\"NodeColor\"][3]\n",
    "        elif nd1_name[:6] == \"DLHojo\":\n",
    "            nd1_color = flag_master[\"NodeColor\"][4]\n",
    "        elif nd1_name[:7] == \"UserAns\":\n",
    "            nd1_color = flag_master[\"NodeColor\"][5]\n",
    "        else:\n",
    "            nd1_color = \"#e5c494\"\n",
    "        \n",
    "        if image_flag:\n",
    "            nd1_image = node_df['ImageURL'][i]\n",
    "            network.add_node(n_id=nd1_id, label=nd1_name, shape='image', image =nd1_image)\n",
    "        else:\n",
    "            network.add_node(n_id=nd1_id, label=nd1_name, color=nd1_color)\n",
    "\n",
    "    for i in range(len(connect_df)):\n",
    "        nd1_id = int(connect_df['NodeNum1'][i])\n",
    "        nd2_id = int(connect_df['NodeNum2'][i])\n",
    "        nd_flag = connect_df['FCConnect'][i]\n",
    "\n",
    "        if nd_flag == 1:\n",
    "            edge_color = \"#2c7bb6\"\n",
    "        else:\n",
    "            edge_color = \"#1a9641\"\n",
    "\n",
    "        # network.add_edge(nd1_id, nd2_id, color=edge_color, width = edge_width)\n",
    "        network.add_edge(nd1_id, nd2_id, color=edge_color, width=0.1)\n",
    "    # 指定したファイル名でHTMLを出力。\n",
    "    network.show(f\"./htmls/fc_{fname}_{tp}.html\")\n",
    "\n",
    "    return 0\n",
    "\n",
    "def clustering_and_create_network(fc_df, embdata, embkadai, embcorp, embhojo, sentencedata, sentencekadai, sentencecorp, sentencehojo, seg, num_clusters, network_num):\n",
    "    #k-means法でDLをクラスタリング\n",
    "    vectors = []\n",
    "    for i in range(len(fc_df)):\n",
    "        vectors.append(np.array(fc_df[\"Vector\"][i]))\n",
    "    norm_vectors = normalize(vectors)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "    kmeans.fit(norm_vectors)\n",
    "    fc_df[\"Cluster\"] = kmeans.labels_\n",
    "    fc_emb_df = fc_df[[\"Sentence\", \"Cluster\"]].groupby([\"Cluster\"])[\"Sentence\"].apply('。'.join).reset_index()\n",
    "\n",
    "    emb_list = []\n",
    "    for i in range(len(kmeans.cluster_centers_)):\n",
    "        emb_list.append(model.encode(fc_emb_df[\"Sentence\"][i], convert_to_tensor=True))\n",
    "\n",
    "    # 入力文と検索対象文のベクトル表現の類似度を計算\n",
    "    tgt_cluster_list = []\n",
    "    dl_sentence_list = []\n",
    "    dl_num_list = []\n",
    "    dl_corp_list = []\n",
    "    dl_dtype_list = []\n",
    "    for emb_i in range(len(emb_list)):\n",
    "        embedding = emb_list[emb_i]\n",
    "        \n",
    "        # data\n",
    "        scores = util.pytorch_cos_sim(embedding, embdata)\n",
    "        sorted, indices = scores.sort(descending=True)\n",
    "        for i in range(5):\n",
    "            predicted_idx = int(indices[0][i]) # スコアが最大のインデックスの取得\n",
    "            tgt_cluster_list.append(emb_i)\n",
    "            dl_sentence_list.append(sentencedata[\"sentence\"][predicted_idx][:50])\n",
    "            dl_num_list.append(f\"DLData,{str(i)}\")\n",
    "            dl_dtype_list.append(\"DLData\")\n",
    "            dl_corp_list.append(sentencedata[\"counterpart\"][predicted_idx])\n",
    "\n",
    "        # kadai\n",
    "        scores = util.pytorch_cos_sim(embedding, embkadai)\n",
    "        sorted, indices = scores.sort(descending=True)\n",
    "        for i in range(2):\n",
    "            predicted_idx = int(indices[0][i]) # スコアが最大のインデックスの取得\n",
    "            tgt_cluster_list.append(emb_i)\n",
    "            dl_sentence_list.append(sentencekadai[\"sentence\"][predicted_idx][:50])\n",
    "            dl_num_list.append(f\"DLKadai,{str(i)}\")\n",
    "            dl_dtype_list.append(\"DLKadai\")\n",
    "            dl_corp_list.append(sentencekadai[\"counterpart\"][predicted_idx])\n",
    "            \n",
    "        # corp\n",
    "        scores = util.pytorch_cos_sim(embedding, embcorp)\n",
    "        sorted, indices = scores.sort(descending=True)\n",
    "        for i in range(3):\n",
    "            predicted_idx = int(indices[0][i]) # スコアが最大のインデックスの取得\n",
    "            tgt_cluster_list.append(emb_i)\n",
    "            dl_sentence_list.append(sentencecorp[\"sentence\"][predicted_idx][:50])\n",
    "            dl_num_list.append(f\"DLCorp,{str(i)}\")\n",
    "            dl_dtype_list.append(\"DLCorp\")\n",
    "            dl_corp_list.append(sentencecorp[\"counterpart\"][predicted_idx])\n",
    "            \n",
    "        # hojo\n",
    "        scores = util.pytorch_cos_sim(embedding, embhojo)\n",
    "        sorted, indices = scores.sort(descending=True)\n",
    "        for i in range(2):\n",
    "            predicted_idx = int(indices[0][i]) # スコアが最大のインデックスの取得\n",
    "            tgt_cluster_list.append(emb_i)\n",
    "            dl_sentence_list.append(sentencehojo[\"sentence\"][predicted_idx][:50])\n",
    "            dl_num_list.append(f\"DLHojo,{str(i)}\")\n",
    "            dl_dtype_list.append(\"DLHojo\")\n",
    "            dl_corp_list.append(sentencehojo[\"counterpart\"][predicted_idx])\n",
    "            \n",
    "        # all answer\n",
    "        fc_flt = fc_df[fc_df[\"Cluster\"] == emb_i].reset_index(drop=True)\n",
    "        for fc_i in range(len(fc_flt)):\n",
    "            tgt_cluster_list.append(emb_i)\n",
    "            dl_sentence_list.append(fc_flt[\"Keyword\"][fc_i])\n",
    "            dl_num_list.append(f\"UserAns,{str(emb_i)}\")\n",
    "            dl_dtype_list.append(\"UserAns\")\n",
    "            dl_corp_list.append(fc_flt[\"User\"][fc_i])\n",
    "            \n",
    "            \n",
    "    dl_df = pd.DataFrame({\n",
    "        'Sentence': dl_sentence_list, \n",
    "        'User': dl_num_list, \n",
    "        'Cluster': tgt_cluster_list, \n",
    "        'Corp': dl_corp_list, \n",
    "        'DataType': dl_dtype_list\n",
    "    })\n",
    "    dl_emb_df = dl_df.copy()\n",
    "\n",
    "    # 各クラスタからKeyPhraseを取得\n",
    "    fc_keyword_all = []\n",
    "    fc_users = []\n",
    "    fc_cluster_list = []\n",
    "    fc_vector_list = []\n",
    "    fc_corp_list = []\n",
    "    for i in range(len(fc_emb_df)):\n",
    "        fc_sentence = fc_emb_df[\"Sentence\"][i]\n",
    "        fc_keyword_list = get_results_for_one_text(fc_sentence[:OUTPUT_TOKENS], PROMPT_TEXT_FC)\n",
    "        for kw_i in range(len(fc_keyword_list)):\n",
    "            kw = fc_keyword_list[kw_i].replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "            \n",
    "            # if (len(kw) >= 3) & (len(kw) < 50):\n",
    "            if (len(kw) >= 3):\n",
    "                fc_keyword_all.append(kw)\n",
    "                fc_users.append(\"FC\")\n",
    "                fc_cluster_list.append(i)\n",
    "                fc_vector_list.append(get_sentence_vector(kw))\n",
    "                fc_corp_list.append(\"\")\n",
    "\n",
    "        flt_dl = dl_emb_df[dl_emb_df[\"Cluster\"] == i].reset_index(drop=True)\n",
    "        for dl_i in range(len(flt_dl)):\n",
    "            dl_sentence = flt_dl[\"Sentence\"][dl_i]\n",
    "            dl_user = flt_dl[\"DataType\"][dl_i]\n",
    "            dl_corp = flt_dl[\"Corp\"][dl_i]\n",
    "            if dl_user == \"DLData\":\n",
    "                kw = dl_sentence.strip().replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "                # if (len(kw) >= 3) & (len(kw) < 50):\n",
    "                if (len(kw) >= 3):\n",
    "                    fc_keyword_all.append(kw)\n",
    "                    fc_users.append(dl_user)\n",
    "                    fc_cluster_list.append(i)\n",
    "                    fc_vector_list.append(get_sentence_vector(kw))\n",
    "                    fc_corp_list.append(dl_corp)\n",
    "            elif dl_user == \"DLKadai\":\n",
    "                dl_keyword_list = get_results_for_one_text(dl_sentence[:OUTPUT_TOKENS], PROMPT_TEXT_KADAI_CORP)\n",
    "                for kw_i in range(len(dl_keyword_list)):\n",
    "                    kw = dl_keyword_list[kw_i].replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "                    # if (len(kw) >= 3) & (len(kw) < 50):\n",
    "                    if (len(kw) >= 3):\n",
    "                        fc_keyword_all.append(kw)\n",
    "                        fc_users.append(dl_user)\n",
    "                        fc_cluster_list.append(i)\n",
    "                        fc_vector_list.append(get_sentence_vector(kw))\n",
    "                        fc_corp_list.append(dl_corp)\n",
    "            elif dl_user == \"DLCorp\":\n",
    "                dl_keyword_list = get_results_for_one_text(dl_sentence[:OUTPUT_TOKENS], PROMPT_TEXT_KADAI_CORP)\n",
    "                for kw_i in range(len(dl_keyword_list)):\n",
    "                    kw = dl_keyword_list[kw_i].replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "                    # if (len(kw) >= 3) & (len(kw) < 50):\n",
    "                    if (len(kw) >= 3):\n",
    "                        fc_keyword_all.append(kw)\n",
    "                        fc_users.append(dl_user)\n",
    "                        fc_cluster_list.append(i)\n",
    "                        fc_vector_list.append(get_sentence_vector(kw))\n",
    "                        fc_corp_list.append(dl_corp)\n",
    "            elif dl_user == \"DLHojo\":\n",
    "                kw = dl_sentence.strip().replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "                if (len(kw) >= 3):\n",
    "                    fc_keyword_all.append(kw)\n",
    "                    fc_users.append(dl_user)\n",
    "                    fc_cluster_list.append(i)\n",
    "                    fc_vector_list.append(get_sentence_vector(kw))\n",
    "                    fc_corp_list.append(dl_corp)\n",
    "            elif dl_user == \"UserAns\":\n",
    "                kw = dl_sentence.strip().replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "                # if (len(kw) >= 3) & (len(kw) < 50):\n",
    "                if (len(kw) >= 3):\n",
    "                    fc_keyword_all.append(kw)\n",
    "                    fc_users.append(dl_user)\n",
    "                    fc_cluster_list.append(i)\n",
    "                    fc_vector_list.append(get_sentence_vector(kw))\n",
    "                    fc_corp_list.append(dl_corp)\n",
    "            else:\n",
    "                dl_keyword_list = get_results_for_one_text(dl_sentence[:OUTPUT_TOKENS], PROMPT_TEXT_OTR)\n",
    "                for kw_i in range(len(dl_keyword_list)):\n",
    "                    kw = dl_keyword_list[kw_i].replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\")\n",
    "                    # if (len(kw) >= 3) & (len(kw) < 50):\n",
    "                    if (len(kw) >= 3):\n",
    "                        fc_keyword_all.append(kw)\n",
    "                        fc_users.append(dl_user)\n",
    "                        fc_cluster_list.append(i)\n",
    "                        fc_vector_list.append(get_sentence_vector(kw))\n",
    "                        fc_corp_list.append(dl_corp)\n",
    "\n",
    "    fc_df_base = pd.DataFrame({\n",
    "        'Keyword': fc_keyword_all, \n",
    "        'User': fc_users, \n",
    "        'Cluster': fc_cluster_list, \n",
    "        'Vector': fc_vector_list, \n",
    "        \"Corp\": fc_corp_list\n",
    "    })\n",
    "\n",
    "    vectors = []\n",
    "    for i in range(len(fc_df_base)):\n",
    "        vectors.append(np.array(fc_df_base[\"Vector\"][i]))\n",
    "    norm_vectors = normalize(vectors)\n",
    "    fc_df_base[\"NormVector\"] = [nv for nv in norm_vectors]\n",
    "    fc_df_base[\"UserCluster\"] = [f\"{usr}{clst}\" for usr, clst in zip(fc_df_base[\"User\"], fc_df_base[\"Cluster\"])]\n",
    "    \n",
    "    # FC単体情報\n",
    "    fc_df_onlyfc = fc_df_base[fc_df_base[\"User\"] == \"FC\"].reset_index(drop=True).copy()\n",
    "    key_pair_list_onlyfc = list(itertools.combinations(fc_df_onlyfc[\"Keyword\"], 2))\n",
    "    key1_list = []\n",
    "    key2_list = []\n",
    "    sim_list = []\n",
    "    for i in range(len(key_pair_list_onlyfc)):\n",
    "        key1 = key_pair_list_onlyfc[i][0]\n",
    "        key2 = key_pair_list_onlyfc[i][1]\n",
    "        if fc_df_onlyfc[fc_df_onlyfc[\"Keyword\"] == key1].reset_index(drop=True)[\"UserCluster\"][0] != fc_df_onlyfc[fc_df_onlyfc[\"Keyword\"] == key2].reset_index(drop=True)[\"UserCluster\"][0]:\n",
    "            vec1 = fc_df_onlyfc[fc_df_onlyfc[\"Keyword\"] == key1].reset_index(drop=True)[\"NormVector\"][0]\n",
    "            vec2 = fc_df_onlyfc[fc_df_onlyfc[\"Keyword\"] == key2].reset_index(drop=True)[\"NormVector\"][0]\n",
    "            sim_key1_key2 = get_similarity(vec1, vec2)\n",
    "            key1_list.append(key1)\n",
    "            key2_list.append(key2)\n",
    "            sim_list.append(sim_key1_key2)\n",
    "\n",
    "    key_pair_df_onlyfc = pd.DataFrame({\n",
    "        'key1': key1_list, \n",
    "        'key2': key2_list, \n",
    "        'similarity': sim_list\n",
    "    })\n",
    "    print(len(key_pair_df_onlyfc))\n",
    "    \n",
    "    # FC&DL両方\n",
    "    fc_df_all = fc_df_base.copy()\n",
    "    key_pair_list_all = list(itertools.combinations(fc_df_all[\"Keyword\"], 2))\n",
    "    key1_list = []\n",
    "    key2_list = []\n",
    "    sim_list = []\n",
    "    for i in range(len(key_pair_list_all)):\n",
    "        key1 = key_pair_list_all[i][0]\n",
    "        key2 = key_pair_list_all[i][1]\n",
    "        if fc_df_all[fc_df_all[\"Keyword\"] == key1].reset_index(drop=True)[\"UserCluster\"][0] != fc_df_all[fc_df_all[\"Keyword\"] == key2].reset_index(drop=True)[\"UserCluster\"][0]:\n",
    "            vec1 = fc_df_all[fc_df_all[\"Keyword\"] == key1].reset_index(drop=True)[\"NormVector\"][0]\n",
    "            vec2 = fc_df_all[fc_df_all[\"Keyword\"] == key2].reset_index(drop=True)[\"NormVector\"][0]\n",
    "            sim_key1_key2 = get_similarity(vec1, vec2)\n",
    "            key1_list.append(key1)\n",
    "            key2_list.append(key2)\n",
    "            sim_list.append(sim_key1_key2)\n",
    "\n",
    "    key_pair_df_all = pd.DataFrame({\n",
    "        'key1': key1_list, \n",
    "        'key2': key2_list, \n",
    "        'similarity': sim_list\n",
    "    })\n",
    "    print(len(key_pair_df_all))\n",
    "\n",
    "    # HTMLを作成\n",
    "    # create_fcdl_network(fc_df_onlyfc, key_pair_df_onlyfc, False, seg, \"onlyfc\", fc_emb_df, network_num, fc_df)\n",
    "    create_fcdl_network(fc_df_all, key_pair_df_all, False, seg, \"fcdl\", fc_emb_df, network_num, fc_df)\n",
    "    # create_fcdl_network(fc_df_onlyfc, key_pair_df_onlyfc, True, seg, \"onlyfc_img\", fc_emb_df, network_num, fc_df)\n",
    "    # create_fcdl_network(fc_df_all, key_pair_df_all, True, seg, \"fcdl_img\", fc_emb_df, network_num, fc_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11494e-0efe-4ec5-a607-8b360c86f0dc",
   "metadata": {},
   "source": [
    "# 課題データセットの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18939dc-c2b2-4fab-a748-f81ddca40fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>counterpart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>横浜市中央児童相談所の環境改善のための連携について（PDF：307KB）, 横浜市児童相談所...</td>\n",
       "      <td>横浜市こども青少年局中央児童相談所庶務係</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ＨＩＶ・エイズ、性感染症検査等の普及啓発（PDF：758KB）, ＨＩＶ／エイズの感染経路は...</td>\n",
       "      <td>横浜市健康福祉局健康安全課結核・エイズ担当</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>「市民と企業によるまちづくり」\\r\\n協働による地域の課題解決・魅力向上のための施設（ハード...</td>\n",
       "      <td>横浜市都市整備局地域まちづくり課</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>「横浜消防」のＰＲ・ブランディングと市民の防災・減災意識の啓発（PDF：329KB）, 大地...</td>\n",
       "      <td>横浜市消防局総務部企画課</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>市民の読書活動の推進のための連携, 読書活動は言葉を学び、感性を磨き、表現力を高め、創造力を...</td>\n",
       "      <td>横浜市教育委員会事務局生涯学習文化財課</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence            counterpart\n",
       "23  横浜市中央児童相談所の環境改善のための連携について（PDF：307KB）, 横浜市児童相談所...   横浜市こども青少年局中央児童相談所庶務係\n",
       "24  ＨＩＶ・エイズ、性感染症検査等の普及啓発（PDF：758KB）, ＨＩＶ／エイズの感染経路は...  横浜市健康福祉局健康安全課結核・エイズ担当\n",
       "25  「市民と企業によるまちづくり」\\r\\n協働による地域の課題解決・魅力向上のための施設（ハード...       横浜市都市整備局地域まちづくり課\n",
       "26  「横浜消防」のＰＲ・ブランディングと市民の防災・減災意識の啓発（PDF：329KB）, 大地...           横浜市消防局総務部企画課\n",
       "27  市民の読書活動の推進のための連携, 読書活動は言葉を学び、感性を磨き、表現力を高め、創造力を...    横浜市教育委員会事務局生涯学習文化財課"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kadai_df = pd.read_csv(\"yokohama_kyoso_theme.csv\")\n",
    "kadai_df.columns = [\"kid\", \"theme\", \"gaiyou\", \"shokan\"]\n",
    "kadai_df[\"fulltext\"] = [f\"{tm}, {gy}\" for tm, gy in zip(kadai_df[\"theme\"], kadai_df[\"gaiyou\"])]\n",
    "\n",
    "sentences_kadai = []\n",
    "for add_s in kadai_df[\"fulltext\"]:\n",
    "    sentences_kadai.append(add_s.strip())\n",
    "    \n",
    "sentences_kadai_df = pd.DataFrame({\n",
    "    \"sentence\": sentences_kadai\n",
    "})\n",
    "sentences_kadai_df[\"counterpart\"] = kadai_df[\"shokan\"]\n",
    "\n",
    "#Compute embeddings\n",
    "if False:\n",
    "    print(\"Start calc embeddings\")\n",
    "    print(datetime.datetime.now())\n",
    "    embeddings_kadai = model.encode(sentences_kadai, convert_to_tensor=True)\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    # pickle化してファイルに書き込み\n",
    "    with open('dataleaves_kadai.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings_kadai, f)\n",
    "else:\n",
    "    with open('dataleaves_kadai.pkl', 'rb') as f:\n",
    "        embeddings_kadai = pickle.load(f)\n",
    "\n",
    "print(len(embeddings_kadai))\n",
    "print(len(sentences_kadai_df))\n",
    "sentences_kadai_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bfdd3f-0789-4752-becb-4729ca3e6862",
   "metadata": {},
   "source": [
    "# 企業データセットの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d818887f-ce9b-40ce-8638-c77e99ac30fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466\n",
      "466\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>counterpart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>横浜市内を中心に公共工事、土木舗装工事を主としています。</td>\n",
       "      <td>和紘建設</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>総合建設業</td>\n",
       "      <td>渡辺組</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>nan</td>\n",
       "      <td>渡辺商事</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>台風・地震・大雨等の天災時にも直ちに出動できる態勢を整えており、横浜市の“災害登録業者”にも...</td>\n",
       "      <td>綿貫建設</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>化粧品の製造卸</td>\n",
       "      <td>ワミレスコスメティックス</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence   counterpart\n",
       "461                       横浜市内を中心に公共工事、土木舗装工事を主としています。          和紘建設\n",
       "462                                              総合建設業           渡辺組\n",
       "463                                                nan          渡辺商事\n",
       "464  台風・地震・大雨等の天災時にも直ちに出動できる態勢を整えており、横浜市の“災害登録業者”にも...          綿貫建設\n",
       "465                                            化粧品の製造卸  ワミレスコスメティックス"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp_df = pd.read_csv(\"yokohama_chiikikouken.csv\")\n",
    "corp_df[\"fulltext\"] = [f\"{pr}, {jg}, {tk}\" for pr, jg, tk in zip(corp_df[\"PR\"], corp_df[\"jigyou\"], corp_df[\"torikumi\"])]\n",
    "corp_df[\"fulltext\"] = [st.replace(\"nan, \", \"\") for st in corp_df[\"fulltext\"]]\n",
    "corp_df[\"fulltext\"] = [st.replace(\", nan\", \"\") for st in corp_df[\"fulltext\"]]\n",
    "\n",
    "sentences_corp = []\n",
    "for add_s in corp_df[\"fulltext\"]:\n",
    "    sentences_corp.append(add_s.strip())\n",
    "    \n",
    "sentences_corp_df = pd.DataFrame({\n",
    "    \"sentence\": sentences_corp\n",
    "})\n",
    "sentences_corp_df[\"counterpart\"] = [st.strip().replace(\"株式会社\", \"\").replace(\"ホールディングス\", \"\") for st in corp_df[\"corpname\"]]\n",
    "\n",
    "#Compute embeddings\n",
    "if False:\n",
    "    print(\"Start calc embeddings\")\n",
    "    print(datetime.datetime.now())\n",
    "    embeddings_corp = model.encode(sentences_corp, convert_to_tensor=True)\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    # pickle化してファイルに書き込み\n",
    "    with open('dataleaves_corp.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings_corp, f)\n",
    "else:\n",
    "    with open('dataleaves_corp.pkl', 'rb') as f:\n",
    "        embeddings_corp = pickle.load(f)\n",
    "\n",
    "print(len(embeddings_corp))\n",
    "print(len(sentences_corp_df))\n",
    "sentences_corp_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055ef0e-6826-4c64-b47f-2e09e1777a03",
   "metadata": {},
   "source": [
    "# 補助金・助成金データの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ca1907-567d-46df-bc6c-dc5885a6e5cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "79\n",
      "    data_number                             fulltext       institute  \\\n",
      "74           75  ('横浜市：2020年度 U39アーティスト・フェローシップ助成',)     横浜市芸術文化振興財団   \n",
      "75           76        ('横浜市：クリエイティブ・インクルージョン活動助成',)     横浜市芸術文化振興財団   \n",
      "76           77                ('LIP. 横浜 トライアル助成金',)  木原記念横浜生命科学振興財団   \n",
      "77           78              ('「横浜市被災中小企業復旧支援補助金」',)             横浜市   \n",
      "78           79               ('横浜市 小規模事業者設備投資助成金',)             横浜市   \n",
      "\n",
      "                          datahead  \n",
      "74  横浜市：2020年度 U39アーティスト・フェローシップ助成  \n",
      "75        横浜市：クリエイティブ・インクルージョン活動助成  \n",
      "76                LIP. 横浜 トライアル助成金  \n",
      "77              「横浜市被災中小企業復旧支援補助金」  \n",
      "78               横浜市 小規模事業者設備投資助成金  \n",
      "79\n",
      "79\n",
      "    data_number                                           fulltext  \\\n",
      "74           75  横浜市：2020年度 U39アーティスト・フェローシップ助成 横浜市：2020年度 U39ア...   \n",
      "75           76  横浜市：クリエイティブ・インクルージョン活動助成 横浜市：クリエイティブ・インクルージョン活...   \n",
      "76           77  LIP. 横浜 トライアル助成金 LIP. 横浜 トライアル助成金 健康・医療分野での試作品...   \n",
      "77           78  「横浜市被災中小企業復旧支援補助金」 「横浜市被災中小企業復旧支援補助金」 横浜市では、令和...   \n",
      "78           79  横浜市 小規模事業者設備投資助成金 横浜市 小規模事業者設備投資助成金 横浜市内で事業を営む...   \n",
      "\n",
      "         institute                        datahead  \n",
      "74     横浜市芸術文化振興財団  横浜市：2020年度 U39アーティスト・フェローシップ助成  \n",
      "75     横浜市芸術文化振興財団        横浜市：クリエイティブ・インクルージョン活動助成  \n",
      "76  木原記念横浜生命科学振興財団                LIP. 横浜 トライアル助成金  \n",
      "77             横浜市              「横浜市被災中小企業復旧支援補助金」  \n",
      "78             横浜市               横浜市 小規模事業者設備投資助成金  \n"
     ]
    }
   ],
   "source": [
    "hojo_yoko = pd.read_csv(\"yokohama_hojokin_header.csv\")\n",
    "hojo_yoko[\"fulltext\"] = [f\"{dh}\" for dh in zip(hojo_yoko[\"datahead\"])]\n",
    "hojo_yoko = hojo_yoko[[\"cid\", \"fulltext\", \"institute\", \"datahead\"]].copy()\n",
    "hojo_yoko.columns = [\"data_number\", \"fulltext\", \"institute\", \"datahead\"]\n",
    "\n",
    "hojo_yoko_desc = pd.read_csv(\"yokohama_hojokin_name.csv\")\n",
    "hojo_yoko_desc[\"fulltext\"] = [f\"{dh} {dn} {obj} {kh}\" for dh, dn, obj, kh in zip(hojo_yoko_desc[\"datahead\"], hojo_yoko_desc[\"dataname\"], hojo_yoko_desc[\"object\"], hojo_yoko_desc[\"keihi\"])]\n",
    "hojo_yoko_desc = hojo_yoko_desc[[\"cid\", \"fulltext\", \"institute\", \"datahead\"]].copy()\n",
    "hojo_yoko_desc.columns = [\"data_number\", \"fulltext\", \"institute\", \"datahead\"]\n",
    "\n",
    "sentences_hojo = []\n",
    "for add_s in hojo_yoko[\"fulltext\"]:\n",
    "    sentences_hojo.append(add_s.strip())\n",
    "sentences_hojo_df = pd.DataFrame({\n",
    "    \"sentence\": sentences_hojo\n",
    "})\n",
    "sentences_hojo_df[\"counterpart\"] = [st.strip() for st in hojo_yoko[\"institute\"]]\n",
    "\n",
    "sentences_hojo_desc = []\n",
    "for add_s in hojo_yoko_desc[\"fulltext\"]:\n",
    "    sentences_hojo_desc.append(add_s.strip())\n",
    "sentences_hojo_desc_df = pd.DataFrame({\n",
    "    \"sentence\": sentences_hojo_desc\n",
    "})\n",
    "sentences_hojo_desc_df[\"counterpart\"] = [st.strip() for st in hojo_yoko_desc[\"institute\"]]\n",
    "\n",
    "#Compute embeddings\n",
    "if False: \n",
    "# if True:\n",
    "    print(\"Start calc embeddings\")\n",
    "    print(datetime.datetime.now())\n",
    "    embeddings_hojo = model.encode(sentences_hojo, convert_to_tensor=True)\n",
    "    embeddings_hojo_desc = model.encode(sentences_hojo_desc, convert_to_tensor=True)\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    # pickle化してファイルに書き込み\n",
    "    with open('dataleaves_hojo.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings_hojo, f)\n",
    "    with open('dataleaves_hojo_desc.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings_hojo_desc, f)\n",
    "else:\n",
    "    with open('dataleaves_hojo.pkl', 'rb') as f:\n",
    "        embeddings_hojo = pickle.load(f)    \n",
    "    with open('dataleaves_hojo_desc.pkl', 'rb') as f:\n",
    "        embeddings_hojo_desc = pickle.load(f)\n",
    "        \n",
    "sentences_hojo_df[\"sentence\"] = hojo_yoko[\"datahead\"]\n",
    "sentences_hojo_desc_df[\"sentence\"] = hojo_yoko_desc[\"datahead\"]\n",
    "print(len(embeddings_hojo))\n",
    "print(len(sentences_hojo_df))\n",
    "print(hojo_yoko.tail())\n",
    "print(len(embeddings_hojo_desc))\n",
    "print(len(sentences_hojo_desc_df))\n",
    "print(hojo_yoko_desc.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f561f-aae9-4a66-a230-e3098b72bcdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# データジャケットの内容をベクトルとして保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "721f4600-6e6f-43aa-931d-a0b1fab997fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n",
      "544\n",
      "     data_number                            fulltext creator\n",
      "539          540   ('   平成２９年１月 市場月報       XLS   ',)   運営調整課\n",
      "540          541  ('   平成２８年１２月 市場月報       XLS   ',)   運営調整課\n",
      "541          542  ('   平成２８年１０月 市場月報       XLS   ',)   運営調整課\n",
      "542          543   ('   平成２８年９月 市場月報       XLS   ',)   運営調整課\n",
      "543          544   ('   平成２８年８月 市場月報       XLS   ',)   運営調整課\n",
      "14208\n",
      "14208\n",
      "       data_number                                           fulltext creator\n",
      "14203        14204  ('   平成２８年８月 市場月報       XLS          鳥卵部\\u3000...   運営調整課\n",
      "14204        14205  ('   平成２８年８月 市場月報       XLS          鳥卵部\\u3000...   運営調整課\n",
      "14205        14206  ('   平成２８年８月 市場月報       XLS          鳥卵部\\u3000...   運営調整課\n",
      "14206        14207  ('   平成２８年８月 市場月報       XLS          鳥卵部\\u3000...   運営調整課\n",
      "14207        14208  ('   平成２８年８月 市場月報       XLS          食肉部\\u3000...   運営調整課\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    dls = pd.read_csv(\"datajacket.csv\", usecols=[\"ID\", \"title\", \"outline\", \"collecting_cost\", \"sharing_policy\", \"type\", \"variable\", \"analysis\", \"outcome\", \"anticipation\", \"comments\", \"wanted\"])\n",
    "    dls[\"fulltext\"] = [f\"{ttl}, {ol}, {v}\" for ttl, ol, v in zip(dls[\"title\"], dls[\"outline\"], dls[\"variable\"])]\n",
    "\n",
    "    sentences = []\n",
    "    for add_s in dls[\"fulltext\"]:\n",
    "        sentences.append(add_s.strip())\n",
    "\n",
    "    #Compute embeddings\n",
    "    if False:\n",
    "        print(\"Start calc embeddings\")\n",
    "        print(datetime.datetime.now())\n",
    "        embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "        # pickle化してファイルに書き込み\n",
    "        with open('dataleaves.pkl', 'wb') as f:\n",
    "            pickle.dump(embeddings, f)\n",
    "    else:\n",
    "        with open('dataleaves.pkl', 'rb') as f:\n",
    "            embeddings = pickle.load(f)\n",
    "else:\n",
    "    dls_yoko = pd.read_csv(\"yokohama_opendata_header.csv\")\n",
    "    dls_yoko[\"fulltext\"] = [f\"{dh}\" for dh in zip(dls_yoko[\"datahead\"])]\n",
    "    dls_yoko = dls_yoko[[\"cid\", \"fulltext\", \"creator\"]].copy()\n",
    "    dls_yoko.columns = [\"data_number\", \"fulltext\", \"creator\"]\n",
    "\n",
    "    dls_yoko_desc = pd.read_csv(\"yokohama_opendata_name.csv\")\n",
    "    dls_yoko_desc[\"fulltext\"] = [f\"{dn}\" for dn in zip(dls_yoko_desc[\"dataname\"])]\n",
    "    dls_yoko_desc = dls_yoko_desc[[\"cid\", \"fulltext\", \"creator\"]].copy()\n",
    "    dls_yoko_desc.columns = [\"data_number\", \"fulltext\", \"creator\"]\n",
    "\n",
    "    sentences_yoko = []\n",
    "    for add_s in dls_yoko[\"fulltext\"]:\n",
    "        sentences_yoko.append(add_s.strip())\n",
    "    sentences_yoko_df = pd.DataFrame({\n",
    "        \"sentence\": sentences_yoko\n",
    "    })\n",
    "    sentences_yoko_df[\"counterpart\"] = [st.strip() for st in dls_yoko[\"creator\"]]\n",
    "\n",
    "    sentences_yoko_desc = []\n",
    "    for add_s in dls_yoko_desc[\"fulltext\"]:\n",
    "        sentences_yoko_desc.append(add_s.strip())\n",
    "    sentences_yoko_desc_df = pd.DataFrame({\n",
    "        \"sentence\": sentences_yoko_desc\n",
    "    })\n",
    "    sentences_yoko_desc_df[\"counterpart\"] = [st.strip() for st in dls_yoko_desc[\"creator\"]]\n",
    "\n",
    "    #Compute embeddings\n",
    "    # if False: \n",
    "    if False:\n",
    "        print(\"Start calc embeddings\")\n",
    "        print(datetime.datetime.now())\n",
    "        embeddings_yoko = model.encode(sentences_yoko, convert_to_tensor=True)\n",
    "        embeddings_yoko_desc = model.encode(sentences_yoko_desc, convert_to_tensor=True)\n",
    "        print(datetime.datetime.now())\n",
    "\n",
    "        # pickle化してファイルに書き込み\n",
    "        with open('dataleaves_yoko.pkl', 'wb') as f:\n",
    "            pickle.dump(embeddings_yoko, f)\n",
    "        with open('dataleaves_yoko_desc.pkl', 'wb') as f:\n",
    "            pickle.dump(embeddings_yoko_desc, f)\n",
    "    else:\n",
    "        with open('dataleaves_yoko.pkl', 'rb') as f:\n",
    "            embeddings_yoko = pickle.load(f)    \n",
    "        with open('dataleaves_yoko_desc.pkl', 'rb') as f:\n",
    "            embeddings_yoko_desc = pickle.load(f)\n",
    "    print(len(embeddings_yoko))\n",
    "    print(len(sentences_yoko_df))\n",
    "    print(dls_yoko.tail())\n",
    "    print(len(embeddings_yoko_desc))\n",
    "    print(len(sentences_yoko_desc_df))\n",
    "    print(dls_yoko_desc.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da3d72-2605-490a-bc4c-4f1f3b32bc75",
   "metadata": {},
   "source": [
    "# FC登録"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca999f13-730e-462b-b8ba-eef53b22d1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Qst,（１）あなたが創りたい未来の横浜とは、どのようなものですか？,自由コメント',\n",
       " 'Qst,（２）創りたい未来の実現のため、あなたはコンソーシアム内でどのような役割を果たしたいですか？,自由コメント',\n",
       " 'Qst,（３）その役割が果たせたかどうかを、どのような指標で測ることができると思いますか？,自由コメント',\n",
       " 'Qst,（４）今後に向け、こういうことが出来たらいいな、やってみたいな、という事業あるいは会議体等があれば自由にご回答ください。,自由コメント']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qst_goal = f\"\"\"\n",
    "    みなさんが創りたい未来の横浜 、果たしたい役割、 コンソーシアムとしてやりたいことについてお聞かせください。\n",
    "\"\"\"\n",
    "\n",
    "qst_list = [\n",
    "    \"Qst,（１）あなたが創りたい未来の横浜とは、どのようなものですか？,自由コメント\", \n",
    "    \"Qst,（２）創りたい未来の実現のため、あなたはコンソーシアム内でどのような役割を果たしたいですか？,自由コメント\", \n",
    "    \"Qst,（３）その役割が果たせたかどうかを、どのような指標で測ることができると思いますか？,自由コメント\", \n",
    "    \"Qst,（４）今後に向け、こういうことが出来たらいいな、やってみたいな、という事業あるいは会議体等があれば自由にご回答ください。,自由コメント\"\n",
    "]\n",
    "qst_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7895d941-08e9-465c-bef5-7dcc2c812e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tstamp</th>\n",
       "      <th>mail</th>\n",
       "      <th>ans1</th>\n",
       "      <th>ans2</th>\n",
       "      <th>ans3</th>\n",
       "      <th>ans4</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023/10/22 10:17:43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>横浜を、地域や社会のためになる活動をやりたいと思った人が、自由に相談し、仲間を見つけ、支援者...</td>\n",
       "      <td>事業提案者の理念、チーム、スキル、事業の収益可能性、支援後の効果検証およびモニタリング等の各...</td>\n",
       "      <td>事業設立数、黒字事業数（含む社会的インパクト）、黒字額（含む社会的インパクト）、KPI達成進...</td>\n",
       "      <td>皆さんがやりたいこと、やってみたいことを自由に投稿頂き、あれこれ議論できるようなアイデアサン...</td>\n",
       "      <td>TA前川</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023/10/22 15:06:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>市民、企業、行政が一体となり、高度な自治を実現している街</td>\n",
       "      <td>行政と企業の対話によるオープンイノベーションの成功事例の開拓と、キャリア教育の一環としての子...</td>\n",
       "      <td>行政と企業の共創による事業取り組み数・売上伸び率・行政の生産性向上指数など、小中高生の自己有...</td>\n",
       "      <td>ケースメソッドによる共創創発ワークショプ、子どもと横浜型地域貢献企業が一緒に参加する共創ダイアログ</td>\n",
       "      <td>emo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023/10/22 19:49:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>誰もが自分らしさを活かし、自然や人と関わりながら楽しく優しく共生している、コンヴィヴィアルなまち</td>\n",
       "      <td>情報発信（市外・海外への発信も含めて）。横浜市内各地で展開されている取り組みの価値を可視化し...</td>\n",
       "      <td>記事数、Webサイトへの訪問者数、記事によって感謝された数</td>\n",
       "      <td>「循環経済とケア」をテーマとするイベントや事業</td>\n",
       "      <td>Yu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023/10/23 8:36:28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>自然と調和しながら、そこに暮らす人が「生きがい」「やりがい」を感じながら、フォーマルなサービ...</td>\n",
       "      <td>団地という多種多様な人々が暮らす場で、住民の声を聞き、それを行政や企業と共有しながら、実走で...</td>\n",
       "      <td>そこに暮らす人々の定量的な幸福度の調査を継続して行いデータ化する。</td>\n",
       "      <td>今、バラバラに動いている様々な取り組みやプロジェクトを点から面に変えていきたいと思います。</td>\n",
       "      <td>一般社団法人団地暮らしの共創　小柴健一</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023/10/23 10:14:42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>活力ある企業活動が優れた雇用環境を市内外の人に提供し、産学官による先進的な取組があふれているまち</td>\n",
       "      <td>行政と企業（と大学）が活発なオープンイノベーションに取り組める場の提供と投資の蓄積</td>\n",
       "      <td>オープンイノベーションの件数と企業の投資額や雇用の実績（報告ベースにはなりますが）</td>\n",
       "      <td>産学官のオープンイノベーションの場は既にあるので、そこと共創コンソーシアムとの連携の場、さら...</td>\n",
       "      <td>きんちゃん</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023/10/23 11:16:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>目の前の課題解決に忙しくするのではなく、先進的なことや創造的なことに人々の時間やお金を割くこ...</td>\n",
       "      <td>20代という立場から、自分の持っている価値観や希望、理想を積極的に発信する。</td>\n",
       "      <td>市民意識調査（未来に対する希望・不安を問う設問の回答）、選挙の投票率、人口増加数、市内の世帯...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Circular Yokohama（ハーチ株式会社）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023/10/24 8:56:06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>良き新らしきものが良き古きものから自然に成長してくるような状況</td>\n",
       "      <td>良き新しきもの、良き古きものを探す遊歩者、それらを繋ぐ篩(ふるい)としての役割</td>\n",
       "      <td>創造的な勉強会の数、創造的なまちあるきの実施回数、継続的な対話の場の質と数</td>\n",
       "      <td>いろんな局の守備範囲をまたいだ継続的な勉強会。具体的なフィールドをみなで観て考える会。市役所...</td>\n",
       "      <td>藤原徹平</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023/10/24 11:48:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100年後の未来を見据え、子供達の礎となる利他の精神と農福スポーツ連携したまち。平時でも災害...</td>\n",
       "      <td>新しい事業を創出し、それを見える化しながら認知拡大、資金調達、事業支援を持続的に実施する。（...</td>\n",
       "      <td>コンソーシアムの事業として、竹山団地やすすきの団地で生まれた事業が基軸となり、他の地域で事業...</td>\n",
       "      <td>企業はふるさと納税獲得に向けた取り組みを団地をキーワードにタブロイド紙を作成しています。行政...</td>\n",
       "      <td>NPO法人KUSC・神奈川大学サッカー部藤森</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023/10/24 16:52:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>自立型経済都市として自立し、愛を持って人や物とか関わりを持てる街</td>\n",
       "      <td>共創案件のブリッジ人材</td>\n",
       "      <td>自立事業としての案件成功数、経済的（雇用率等含む）な結果</td>\n",
       "      <td>ポケモンや、ガンダム等のインパクトのある事業（関係人口の増加含む）</td>\n",
       "      <td>コミュニティデザイン・ラボ　町山</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023/10/24 16:56:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18区それぞれに特徴があるので、中区、西区の中心だけでなく、その彩り豊かな特徴を出し合えるまち</td>\n",
       "      <td>それぞれに特徴を活かしあえる情報発信基地</td>\n",
       "      <td>Well-being指標で図る。市民総幸福量の創設など。</td>\n",
       "      <td>人口減少社会でも、明るく仲良く、行ってみたい、住んでみたいまちとなるためにも、みんながまちづ...</td>\n",
       "      <td>とつかリビングラボ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023/10/25 5:32:18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>企業と市民が共に助け合い地域の課題に立ち向かう先駆的な街</td>\n",
       "      <td>自治体ニーズと企業シーズの触媒役</td>\n",
       "      <td>実証実験数、寄付件数、寄付金額</td>\n",
       "      <td>触媒役をできる人財育成。具体的には、中小企業診断士資格保有者で企業内にいる（独立していない）...</td>\n",
       "      <td>ミッキー</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023/10/25 9:24:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>エコロジーとテクノロジーとの共生、人と人のコミュニケーションリテラシーの向上</td>\n",
       "      <td>リビングラボ全体のボトムアップとリブランディング、CSRのように一般化させるために地域貢献企...</td>\n",
       "      <td>スキームが構築できた時</td>\n",
       "      <td>NaN</td>\n",
       "      <td>いよいよ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tstamp  mail  \\\n",
       "0   2023/10/22 10:17:43   NaN   \n",
       "1   2023/10/22 15:06:57   NaN   \n",
       "2   2023/10/22 19:49:45   NaN   \n",
       "3    2023/10/23 8:36:28   NaN   \n",
       "4   2023/10/23 10:14:42   NaN   \n",
       "5   2023/10/23 11:16:24   NaN   \n",
       "6    2023/10/24 8:56:06   NaN   \n",
       "7   2023/10/24 11:48:30   NaN   \n",
       "8   2023/10/24 16:52:30   NaN   \n",
       "9   2023/10/24 16:56:37   NaN   \n",
       "10   2023/10/25 5:32:18   NaN   \n",
       "11   2023/10/25 9:24:40   NaN   \n",
       "\n",
       "                                                 ans1  \\\n",
       "0   横浜を、地域や社会のためになる活動をやりたいと思った人が、自由に相談し、仲間を見つけ、支援者...   \n",
       "1                        市民、企業、行政が一体となり、高度な自治を実現している街   \n",
       "2    誰もが自分らしさを活かし、自然や人と関わりながら楽しく優しく共生している、コンヴィヴィアルなまち   \n",
       "3   自然と調和しながら、そこに暮らす人が「生きがい」「やりがい」を感じながら、フォーマルなサービ...   \n",
       "4    活力ある企業活動が優れた雇用環境を市内外の人に提供し、産学官による先進的な取組があふれているまち   \n",
       "5   目の前の課題解決に忙しくするのではなく、先進的なことや創造的なことに人々の時間やお金を割くこ...   \n",
       "6                     良き新らしきものが良き古きものから自然に成長してくるような状況   \n",
       "7   100年後の未来を見据え、子供達の礎となる利他の精神と農福スポーツ連携したまち。平時でも災害...   \n",
       "8                    自立型経済都市として自立し、愛を持って人や物とか関わりを持てる街   \n",
       "9     18区それぞれに特徴があるので、中区、西区の中心だけでなく、その彩り豊かな特徴を出し合えるまち   \n",
       "10                       企業と市民が共に助け合い地域の課題に立ち向かう先駆的な街   \n",
       "11             エコロジーとテクノロジーとの共生、人と人のコミュニケーションリテラシーの向上   \n",
       "\n",
       "                                                 ans2  \\\n",
       "0   事業提案者の理念、チーム、スキル、事業の収益可能性、支援後の効果検証およびモニタリング等の各...   \n",
       "1   行政と企業の対話によるオープンイノベーションの成功事例の開拓と、キャリア教育の一環としての子...   \n",
       "2   情報発信（市外・海外への発信も含めて）。横浜市内各地で展開されている取り組みの価値を可視化し...   \n",
       "3   団地という多種多様な人々が暮らす場で、住民の声を聞き、それを行政や企業と共有しながら、実走で...   \n",
       "4           行政と企業（と大学）が活発なオープンイノベーションに取り組める場の提供と投資の蓄積   \n",
       "5              20代という立場から、自分の持っている価値観や希望、理想を積極的に発信する。   \n",
       "6             良き新しきもの、良き古きものを探す遊歩者、それらを繋ぐ篩(ふるい)としての役割   \n",
       "7   新しい事業を創出し、それを見える化しながら認知拡大、資金調達、事業支援を持続的に実施する。（...   \n",
       "8                                         共創案件のブリッジ人材   \n",
       "9                                それぞれに特徴を活かしあえる情報発信基地   \n",
       "10                                   自治体ニーズと企業シーズの触媒役   \n",
       "11  リビングラボ全体のボトムアップとリブランディング、CSRのように一般化させるために地域貢献企...   \n",
       "\n",
       "                                                 ans3  \\\n",
       "0   事業設立数、黒字事業数（含む社会的インパクト）、黒字額（含む社会的インパクト）、KPI達成進...   \n",
       "1   行政と企業の共創による事業取り組み数・売上伸び率・行政の生産性向上指数など、小中高生の自己有...   \n",
       "2                       記事数、Webサイトへの訪問者数、記事によって感謝された数   \n",
       "3                   そこに暮らす人々の定量的な幸福度の調査を継続して行いデータ化する。   \n",
       "4           オープンイノベーションの件数と企業の投資額や雇用の実績（報告ベースにはなりますが）   \n",
       "5   市民意識調査（未来に対する希望・不安を問う設問の回答）、選挙の投票率、人口増加数、市内の世帯...   \n",
       "6               創造的な勉強会の数、創造的なまちあるきの実施回数、継続的な対話の場の質と数   \n",
       "7   コンソーシアムの事業として、竹山団地やすすきの団地で生まれた事業が基軸となり、他の地域で事業...   \n",
       "8                        自立事業としての案件成功数、経済的（雇用率等含む）な結果   \n",
       "9                        Well-being指標で図る。市民総幸福量の創設など。   \n",
       "10                                    実証実験数、寄付件数、寄付金額   \n",
       "11                                        スキームが構築できた時   \n",
       "\n",
       "                                                 ans4  \\\n",
       "0   皆さんがやりたいこと、やってみたいことを自由に投稿頂き、あれこれ議論できるようなアイデアサン...   \n",
       "1   ケースメソッドによる共創創発ワークショプ、子どもと横浜型地域貢献企業が一緒に参加する共創ダイアログ   \n",
       "2                             「循環経済とケア」をテーマとするイベントや事業   \n",
       "3       今、バラバラに動いている様々な取り組みやプロジェクトを点から面に変えていきたいと思います。   \n",
       "4   産学官のオープンイノベーションの場は既にあるので、そこと共創コンソーシアムとの連携の場、さら...   \n",
       "5                                                 NaN   \n",
       "6   いろんな局の守備範囲をまたいだ継続的な勉強会。具体的なフィールドをみなで観て考える会。市役所...   \n",
       "7   企業はふるさと納税獲得に向けた取り組みを団地をキーワードにタブロイド紙を作成しています。行政...   \n",
       "8                   ポケモンや、ガンダム等のインパクトのある事業（関係人口の増加含む）   \n",
       "9   人口減少社会でも、明るく仲良く、行ってみたい、住んでみたいまちとなるためにも、みんながまちづ...   \n",
       "10  触媒役をできる人財育成。具体的には、中小企業診断士資格保有者で企業内にいる（独立していない）...   \n",
       "11                                                NaN   \n",
       "\n",
       "                         email  \n",
       "0                         TA前川  \n",
       "1                          emo  \n",
       "2                           Yu  \n",
       "3          一般社団法人団地暮らしの共創　小柴健一  \n",
       "4                        きんちゃん  \n",
       "5   Circular Yokohama（ハーチ株式会社）  \n",
       "6                         藤原徹平  \n",
       "7       NPO法人KUSC・神奈川大学サッカー部藤森  \n",
       "8             コミュニティデザイン・ラボ　町山  \n",
       "9                    とつかリビングラボ  \n",
       "10                        ミッキー  \n",
       "11                        いよいよ  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if False:\n",
    "    conn = pyathena.connect(\n",
    "        s3_staging_dir=\"s3://lnext-for-quicksight/athena-cash/\",\n",
    "        region_name=\"ap-northeast-1\"\n",
    "    )\n",
    "\n",
    "    broker = 'kuma'\n",
    "    event = 'kuma_20231117kuma'\n",
    "\n",
    "    # sql = f\"SELECT * FROM AwsDataCatalog.lnext_sandbox.t_tsugou_kpi2 WHERE broker = '{broker}' AND event = '{event}' order by answer_time desc;\"\n",
    "    sql = f\"SELECT * FROM AwsDataCatalog.lnext_sandbox.t_tsugou_kpi2 WHERE broker = '{broker}' order by answer_time desc;\"\n",
    "    opt_df = pd.read_sql(sql, conn)\n",
    "    print(len(opt_df))\n",
    "    conn.close()\n",
    "else:\n",
    "    opt_df = pd.read_csv(\"みんなで創りたい未来の横浜（回答） - フォームの回答 1.csv\")\n",
    "    opt_df.columns = [\"tstamp\", \"mail\", \"ans1\", \"ans2\", \"ans3\", \"ans4\", \"email\"]\n",
    "opt_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d6958f3-f133-477a-ab5c-b1d8011b405c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_cols_df = pd.DataFrame({\"Col\": opt_df.columns})\n",
    "ans_cols_df[\"flag\"] = [1 if (ch[:3] == \"ans\") & (len(ch) < 10) else 0 for ch in ans_cols_df[\"Col\"]]\n",
    "ans_cols_df = ans_cols_df[ans_cols_df[\"flag\"] == 1].reset_index(drop=True)\n",
    "ans_max = len(ans_cols_df)\n",
    "ans_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61199ba-889d-4e84-a170-47e5abbd1ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now calculating 1 of 12's data...\n",
      "Now calculating 2 of 12's data...\n",
      "Now calculating 3 of 12's data...\n",
      "Now calculating 4 of 12's data...\n",
      "Now calculating 5 of 12's data...\n",
      "Now calculating 6 of 12's data...\n",
      "Now calculating 7 of 12's data...\n",
      "Now calculating 8 of 12's data...\n",
      "Now calculating 9 of 12's data...\n",
      "Now calculating 10 of 12's data...\n",
      "Now calculating 11 of 12's data...\n",
      "Now calculating 12 of 12's data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Vector</th>\n",
       "      <th>User</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>横浜を、地域や社会のためになる活動をやりたいと思った人が、自由に相談し、仲間を見つけ、支援者...</td>\n",
       "      <td>[0.15350434, -0.011069527, 0.36629382, -0.2243...</td>\n",
       "      <td>TA前川</td>\n",
       "      <td>1</td>\n",
       "      <td>横浜をチャンスある街にする</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>事業提案者の理念、チーム、スキル、事業の収益可能性、支援後の効果検証およびモニタリング等の各...</td>\n",
       "      <td>[-0.01600974, 0.96617544, 1.1718636, -0.725385...</td>\n",
       "      <td>TA前川</td>\n",
       "      <td>2</td>\n",
       "      <td>事業評価手法の開発運用</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>事業設立数、黒字事業数（含む社会的インパクト）、黒字額（含む社会的インパクト）、KPI達成進...</td>\n",
       "      <td>[0.21427114, 0.2961616, 0.11891878, -0.4338636...</td>\n",
       "      <td>TA前川</td>\n",
       "      <td>3</td>\n",
       "      <td>社会的インパクトを含む黒字事業</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>皆さんがやりたいこと、やってみたいことを自由に投稿頂き、あれこれ議論できるようなアイデアサン...</td>\n",
       "      <td>[-0.015601583, 0.3038959, 0.580638, 0.07355337...</td>\n",
       "      <td>TA前川</td>\n",
       "      <td>4</td>\n",
       "      <td>アイデアサンドボックスの構築</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>市民、企業、行政が一体となり、高度な自治を実現している街</td>\n",
       "      <td>[0.2688053, 0.052769035, 0.8365535, -0.1892348...</td>\n",
       "      <td>emo</td>\n",
       "      <td>1</td>\n",
       "      <td>市民、企業、行政が一体となり、高度な自治を実現している街</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>行政と企業の対話によるオープンイノベーションの成功事例の開拓と、キャリア教育の一環としての子...</td>\n",
       "      <td>[-0.29456797, 0.026520735, 0.7673636, 0.089263...</td>\n",
       "      <td>emo</td>\n",
       "      <td>2</td>\n",
       "      <td>オープンイノベーションと子ども参加</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>行政と企業の共創による事業取り組み数・売上伸び率・行政の生産性向上指数など、小中高生の自己有...</td>\n",
       "      <td>[-0.10903608, 0.5291753, 0.030806696, -0.39204...</td>\n",
       "      <td>emo</td>\n",
       "      <td>3</td>\n",
       "      <td>行政企業の共創による成果</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ケースメソッドによる共創創発ワークショプ、子どもと横浜型地域貢献企業が一緒に参加する共創ダイアログ</td>\n",
       "      <td>[0.18595317, -0.056578077, 0.451056, 0.0625802...</td>\n",
       "      <td>emo</td>\n",
       "      <td>4</td>\n",
       "      <td>子どもと企業の共創ワークショップ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>誰もが自分らしさを活かし、自然や人と関わりながら楽しく優しく共生している、コンヴィヴィアルなまち</td>\n",
       "      <td>[-0.38173732, -0.06489463, 1.0201044, 0.250125...</td>\n",
       "      <td>Yu</td>\n",
       "      <td>1</td>\n",
       "      <td>コンヴィヴィアルな共生まち</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>情報発信（市外・海外への発信も含めて）。横浜市内各地で展開されている取り組みの価値を可視化し...</td>\n",
       "      <td>[0.23779905, -0.11628204, 0.6812859, -0.343407...</td>\n",
       "      <td>Yu</td>\n",
       "      <td>2</td>\n",
       "      <td>横浜市の取り組みを発信する</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>記事数、Webサイトへの訪問者数、記事によって感謝された数</td>\n",
       "      <td>[-0.22223017, 0.6175762, 1.0857452, 0.01081747...</td>\n",
       "      <td>Yu</td>\n",
       "      <td>3</td>\n",
       "      <td>記事数、Webサイトへの訪問者数、記事によって感謝された数</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>「循環経済とケア」をテーマとするイベントや事業</td>\n",
       "      <td>[0.17047112, -0.07413415, 0.9424215, -0.334402...</td>\n",
       "      <td>Yu</td>\n",
       "      <td>4</td>\n",
       "      <td>「循環経済とケア」をテーマとするイベントや事業</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>自然と調和しながら、そこに暮らす人が「生きがい」「やりがい」を感じながら、フォーマルなサービ...</td>\n",
       "      <td>[0.031640965, -0.19801572, 0.5024514, 0.065041...</td>\n",
       "      <td>一般社団法人団地暮らしの共創　小柴健一</td>\n",
       "      <td>1</td>\n",
       "      <td>地域づくりをたすけあいで持続させる</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>団地という多種多様な人々が暮らす場で、住民の声を聞き、それを行政や企業と共有しながら、実走で...</td>\n",
       "      <td>[0.36531875, 0.37158275, 0.48456764, -0.232610...</td>\n",
       "      <td>一般社団法人団地暮らしの共創　小柴健一</td>\n",
       "      <td>2</td>\n",
       "      <td>住民の声を共有し仕組み作り</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>そこに暮らす人々の定量的な幸福度の調査を継続して行いデータ化する。</td>\n",
       "      <td>[0.30118367, 0.5062818, 0.58145624, 0.10106157...</td>\n",
       "      <td>一般社団法人団地暮らしの共創　小柴健一</td>\n",
       "      <td>3</td>\n",
       "      <td>地域の幸福度を調査データ化する</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>今、バラバラに動いている様々な取り組みやプロジェクトを点から面に変えていきたいと思います。</td>\n",
       "      <td>[0.17070527, -0.22292602, 0.8498536, 0.0013588...</td>\n",
       "      <td>一般社団法人団地暮らしの共創　小柴健一</td>\n",
       "      <td>4</td>\n",
       "      <td>様々な取り組みを統合する</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>活力ある企業活動が優れた雇用環境を市内外の人に提供し、産学官による先進的な取組があふれているまち</td>\n",
       "      <td>[-0.23170915, 0.3670838, 0.4376015, -0.5879583...</td>\n",
       "      <td>きんちゃん</td>\n",
       "      <td>1</td>\n",
       "      <td>市内外に優れた雇用環境を提供</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>行政と企業（と大学）が活発なオープンイノベーションに取り組める場の提供と投資の蓄積</td>\n",
       "      <td>[-0.16830447, 0.07773729, 0.5605185, -0.296834...</td>\n",
       "      <td>きんちゃん</td>\n",
       "      <td>2</td>\n",
       "      <td>オープンイノベーションの推進</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>オープンイノベーションの件数と企業の投資額や雇用の実績（報告ベースにはなりますが）</td>\n",
       "      <td>[0.043046813, 0.39170945, 0.26555818, -0.63678...</td>\n",
       "      <td>きんちゃん</td>\n",
       "      <td>3</td>\n",
       "      <td>オープンイノベーションの投資効果</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>産学官のオープンイノベーションの場は既にあるので、そこと共創コンソーシアムとの連携の場、さら...</td>\n",
       "      <td>[-0.33113652, -0.14651328, 0.5758766, -0.33886...</td>\n",
       "      <td>きんちゃん</td>\n",
       "      <td>4</td>\n",
       "      <td>オープンイノベーションの連携場</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>目の前の課題解決に忙しくするのではなく、先進的なことや創造的なことに人々の時間やお金を割くこ...</td>\n",
       "      <td>[0.10956125, -0.2147402, 0.39737117, 0.0227459...</td>\n",
       "      <td>Circular Yokohama（ハーチ株式会社）</td>\n",
       "      <td>1</td>\n",
       "      <td>創造的な街づくりを推進する</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20代という立場から、自分の持っている価値観や希望、理想を積極的に発信する。</td>\n",
       "      <td>[0.40800375, 0.006384422, 0.8019866, -0.236924...</td>\n",
       "      <td>Circular Yokohama（ハーチ株式会社）</td>\n",
       "      <td>2</td>\n",
       "      <td>20代の価値観希望理想を発信する</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>市民意識調査（未来に対する希望・不安を問う設問の回答）、選挙の投票率、人口増加数、市内の世帯...</td>\n",
       "      <td>[0.56484497, -0.751867, 0.5702715, 0.5117423, ...</td>\n",
       "      <td>Circular Yokohama（ハーチ株式会社）</td>\n",
       "      <td>3</td>\n",
       "      <td>市民の意識調査と投票率の状況</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>良き新らしきものが良き古きものから自然に成長してくるような状況</td>\n",
       "      <td>[0.22978468, -0.4031109, 0.9669361, -0.0426368...</td>\n",
       "      <td>藤原徹平</td>\n",
       "      <td>1</td>\n",
       "      <td>古きものから新しきものへの発展</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>良き新しきもの、良き古きものを探す遊歩者、それらを繋ぐ篩(ふるい)としての役割</td>\n",
       "      <td>[-0.09648979, 0.055998657, 1.4428711, 0.317735...</td>\n",
       "      <td>藤原徹平</td>\n",
       "      <td>2</td>\n",
       "      <td>新旧を繋ぐ篩として</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>創造的な勉強会の数、創造的なまちあるきの実施回数、継続的な対話の場の質と数</td>\n",
       "      <td>[-0.45005807, 0.20697588, 0.52582383, 0.335635...</td>\n",
       "      <td>藤原徹平</td>\n",
       "      <td>3</td>\n",
       "      <td>創造的なコミュニティ活動</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>いろんな局の守備範囲をまたいだ継続的な勉強会。具体的なフィールドをみなで観て考える会。市役所...</td>\n",
       "      <td>[-0.06273041, -0.26858333, 0.502122, 0.3729524...</td>\n",
       "      <td>藤原徹平</td>\n",
       "      <td>4</td>\n",
       "      <td>継続的な学びの場：市役所低層部</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>100年後の未来を見据え、子供達の礎となる利他の精神と農福スポーツ連携したまち。平時でも災害...</td>\n",
       "      <td>[-0.20543465, 0.13389634, 0.76032037, 0.226678...</td>\n",
       "      <td>NPO法人KUSC・神奈川大学サッカー部藤森</td>\n",
       "      <td>1</td>\n",
       "      <td>農福スポーツ連携でQOL維持のまち</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>新しい事業を創出し、それを見える化しながら認知拡大、資金調達、事業支援を持続的に実施する。（...</td>\n",
       "      <td>[0.32653117, 0.41252592, 1.0726038, -0.9254759...</td>\n",
       "      <td>NPO法人KUSC・神奈川大学サッカー部藤森</td>\n",
       "      <td>2</td>\n",
       "      <td>新事業の創出支援を実施する</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>コンソーシアムの事業として、竹山団地やすすきの団地で生まれた事業が基軸となり、他の地域で事業...</td>\n",
       "      <td>[0.25647098, 0.19503014, 0.3793733, -0.1442564...</td>\n",
       "      <td>NPO法人KUSC・神奈川大学サッカー部藤森</td>\n",
       "      <td>3</td>\n",
       "      <td>竹山団地すすきの団地事業支援の実績</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>企業はふるさと納税獲得に向けた取り組みを団地をキーワードにタブロイド紙を作成しています。行政...</td>\n",
       "      <td>[0.01223959, 0.7328718, -0.1754158, 0.3445327,...</td>\n",
       "      <td>NPO法人KUSC・神奈川大学サッカー部藤森</td>\n",
       "      <td>4</td>\n",
       "      <td>竹山団地すすきの団地のふるさと納税取り組み</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>自立型経済都市として自立し、愛を持って人や物とか関わりを持てる街</td>\n",
       "      <td>[0.115973815, 0.08454449, 0.667538, 0.10936332...</td>\n",
       "      <td>コミュニティデザイン・ラボ　町山</td>\n",
       "      <td>1</td>\n",
       "      <td>愛を持って自立する街</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>共創案件のブリッジ人材</td>\n",
       "      <td>[0.17846943, 0.26125696, 0.9453256, 0.12821321...</td>\n",
       "      <td>コミュニティデザイン・ラボ　町山</td>\n",
       "      <td>2</td>\n",
       "      <td>共創案件のブリッジ人材</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>自立事業としての案件成功数、経済的（雇用率等含む）な結果</td>\n",
       "      <td>[-0.09243641, 0.7118591, 0.34000805, -0.452339...</td>\n",
       "      <td>コミュニティデザイン・ラボ　町山</td>\n",
       "      <td>3</td>\n",
       "      <td>自立事業としての案件成功数、経済的（雇用率等含む）な結果</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ポケモンや、ガンダム等のインパクトのある事業（関係人口の増加含む）</td>\n",
       "      <td>[-0.023184536, -0.6220377, 0.57217103, -0.5575...</td>\n",
       "      <td>コミュニティデザイン・ラボ　町山</td>\n",
       "      <td>4</td>\n",
       "      <td>インパクトのある事業の紹介</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>18区それぞれに特徴があるので、中区、西区の中心だけでなく、その彩り豊かな特徴を出し合えるまち</td>\n",
       "      <td>[0.7407473, -0.31217045, 0.72125256, 0.1477113...</td>\n",
       "      <td>とつかリビングラボ</td>\n",
       "      <td>1</td>\n",
       "      <td>京都の18区の特徴を楽しむ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>それぞれに特徴を活かしあえる情報発信基地</td>\n",
       "      <td>[0.21762559, -0.029377174, 1.1780553, 0.329879...</td>\n",
       "      <td>とつかリビングラボ</td>\n",
       "      <td>2</td>\n",
       "      <td>それぞれに特徴を活かしあえる情報発信基地</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Well-being指標で図る。市民総幸福量の創設など。</td>\n",
       "      <td>[0.47751266, 0.21097979, 0.7651377, -0.0166073...</td>\n",
       "      <td>とつかリビングラボ</td>\n",
       "      <td>3</td>\n",
       "      <td>Well-being指標で図る。市民総幸福量の創設など。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>人口減少社会でも、明るく仲良く、行ってみたい、住んでみたいまちとなるためにも、みんながまちづ...</td>\n",
       "      <td>[0.15831745, -0.12689763, 0.73806167, 0.061833...</td>\n",
       "      <td>とつかリビングラボ</td>\n",
       "      <td>4</td>\n",
       "      <td>人口減少社会でも明るいまちづくり</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>企業と市民が共に助け合い地域の課題に立ち向かう先駆的な街</td>\n",
       "      <td>[0.023847984, 0.103833035, 0.49484363, -0.5503...</td>\n",
       "      <td>ミッキー</td>\n",
       "      <td>1</td>\n",
       "      <td>企業と市民が共に助け合い地域の課題に立ち向かう先駆的な街</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>自治体ニーズと企業シーズの触媒役</td>\n",
       "      <td>[0.2534478, 0.1634743, 0.83604115, -0.2745389,...</td>\n",
       "      <td>ミッキー</td>\n",
       "      <td>2</td>\n",
       "      <td>自治体ニーズと企業シーズの触媒役</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>実証実験数、寄付件数、寄付金額</td>\n",
       "      <td>[0.2833146, -0.016000703, -0.05473227, -0.0227...</td>\n",
       "      <td>ミッキー</td>\n",
       "      <td>3</td>\n",
       "      <td>実証実験数、寄付件数、寄付金額</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>触媒役をできる人財育成。具体的には、中小企業診断士資格保有者で企業内にいる（独立していない）...</td>\n",
       "      <td>[0.2342671, 0.5240219, 0.7462305, -0.2943097, ...</td>\n",
       "      <td>ミッキー</td>\n",
       "      <td>4</td>\n",
       "      <td>中小企業診断士育成の研修</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>エコロジーとテクノロジーとの共生、人と人のコミュニケーションリテラシーの向上</td>\n",
       "      <td>[-0.060757887, -0.06561157, 0.7962137, -0.2418...</td>\n",
       "      <td>いよいよ</td>\n",
       "      <td>1</td>\n",
       "      <td>エコロジーとテクノロジーの共生</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>リビングラボ全体のボトムアップとリブランディング、CSRのように一般化させるために地域貢献企...</td>\n",
       "      <td>[0.0524387, 0.3335148, 1.0886497, -0.23916577,...</td>\n",
       "      <td>いよいよ</td>\n",
       "      <td>2</td>\n",
       "      <td>地域貢献企業認定制度を作る</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>スキームが構築できた時</td>\n",
       "      <td>[0.110550135, -0.41170192, 1.3172055, 0.057960...</td>\n",
       "      <td>いよいよ</td>\n",
       "      <td>3</td>\n",
       "      <td>スキームが構築できた時</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "0   横浜を、地域や社会のためになる活動をやりたいと思った人が、自由に相談し、仲間を見つけ、支援者...   \n",
       "1   事業提案者の理念、チーム、スキル、事業の収益可能性、支援後の効果検証およびモニタリング等の各...   \n",
       "2   事業設立数、黒字事業数（含む社会的インパクト）、黒字額（含む社会的インパクト）、KPI達成進...   \n",
       "3   皆さんがやりたいこと、やってみたいことを自由に投稿頂き、あれこれ議論できるようなアイデアサン...   \n",
       "4                        市民、企業、行政が一体となり、高度な自治を実現している街   \n",
       "5   行政と企業の対話によるオープンイノベーションの成功事例の開拓と、キャリア教育の一環としての子...   \n",
       "6   行政と企業の共創による事業取り組み数・売上伸び率・行政の生産性向上指数など、小中高生の自己有...   \n",
       "7   ケースメソッドによる共創創発ワークショプ、子どもと横浜型地域貢献企業が一緒に参加する共創ダイアログ   \n",
       "8    誰もが自分らしさを活かし、自然や人と関わりながら楽しく優しく共生している、コンヴィヴィアルなまち   \n",
       "9   情報発信（市外・海外への発信も含めて）。横浜市内各地で展開されている取り組みの価値を可視化し...   \n",
       "10                      記事数、Webサイトへの訪問者数、記事によって感謝された数   \n",
       "11                            「循環経済とケア」をテーマとするイベントや事業   \n",
       "12  自然と調和しながら、そこに暮らす人が「生きがい」「やりがい」を感じながら、フォーマルなサービ...   \n",
       "13  団地という多種多様な人々が暮らす場で、住民の声を聞き、それを行政や企業と共有しながら、実走で...   \n",
       "14                  そこに暮らす人々の定量的な幸福度の調査を継続して行いデータ化する。   \n",
       "15      今、バラバラに動いている様々な取り組みやプロジェクトを点から面に変えていきたいと思います。   \n",
       "16   活力ある企業活動が優れた雇用環境を市内外の人に提供し、産学官による先進的な取組があふれているまち   \n",
       "17          行政と企業（と大学）が活発なオープンイノベーションに取り組める場の提供と投資の蓄積   \n",
       "18          オープンイノベーションの件数と企業の投資額や雇用の実績（報告ベースにはなりますが）   \n",
       "19  産学官のオープンイノベーションの場は既にあるので、そこと共創コンソーシアムとの連携の場、さら...   \n",
       "20  目の前の課題解決に忙しくするのではなく、先進的なことや創造的なことに人々の時間やお金を割くこ...   \n",
       "21             20代という立場から、自分の持っている価値観や希望、理想を積極的に発信する。   \n",
       "22  市民意識調査（未来に対する希望・不安を問う設問の回答）、選挙の投票率、人口増加数、市内の世帯...   \n",
       "23                    良き新らしきものが良き古きものから自然に成長してくるような状況   \n",
       "24            良き新しきもの、良き古きものを探す遊歩者、それらを繋ぐ篩(ふるい)としての役割   \n",
       "25              創造的な勉強会の数、創造的なまちあるきの実施回数、継続的な対話の場の質と数   \n",
       "26  いろんな局の守備範囲をまたいだ継続的な勉強会。具体的なフィールドをみなで観て考える会。市役所...   \n",
       "27  100年後の未来を見据え、子供達の礎となる利他の精神と農福スポーツ連携したまち。平時でも災害...   \n",
       "28  新しい事業を創出し、それを見える化しながら認知拡大、資金調達、事業支援を持続的に実施する。（...   \n",
       "29  コンソーシアムの事業として、竹山団地やすすきの団地で生まれた事業が基軸となり、他の地域で事業...   \n",
       "30  企業はふるさと納税獲得に向けた取り組みを団地をキーワードにタブロイド紙を作成しています。行政...   \n",
       "31                   自立型経済都市として自立し、愛を持って人や物とか関わりを持てる街   \n",
       "32                                        共創案件のブリッジ人材   \n",
       "33                       自立事業としての案件成功数、経済的（雇用率等含む）な結果   \n",
       "34                  ポケモンや、ガンダム等のインパクトのある事業（関係人口の増加含む）   \n",
       "35    18区それぞれに特徴があるので、中区、西区の中心だけでなく、その彩り豊かな特徴を出し合えるまち   \n",
       "36                               それぞれに特徴を活かしあえる情報発信基地   \n",
       "37                       Well-being指標で図る。市民総幸福量の創設など。   \n",
       "38  人口減少社会でも、明るく仲良く、行ってみたい、住んでみたいまちとなるためにも、みんながまちづ...   \n",
       "39                       企業と市民が共に助け合い地域の課題に立ち向かう先駆的な街   \n",
       "40                                   自治体ニーズと企業シーズの触媒役   \n",
       "41                                    実証実験数、寄付件数、寄付金額   \n",
       "42  触媒役をできる人財育成。具体的には、中小企業診断士資格保有者で企業内にいる（独立していない）...   \n",
       "43             エコロジーとテクノロジーとの共生、人と人のコミュニケーションリテラシーの向上   \n",
       "44  リビングラボ全体のボトムアップとリブランディング、CSRのように一般化させるために地域貢献企...   \n",
       "45                                        スキームが構築できた時   \n",
       "\n",
       "                                               Vector  \\\n",
       "0   [0.15350434, -0.011069527, 0.36629382, -0.2243...   \n",
       "1   [-0.01600974, 0.96617544, 1.1718636, -0.725385...   \n",
       "2   [0.21427114, 0.2961616, 0.11891878, -0.4338636...   \n",
       "3   [-0.015601583, 0.3038959, 0.580638, 0.07355337...   \n",
       "4   [0.2688053, 0.052769035, 0.8365535, -0.1892348...   \n",
       "5   [-0.29456797, 0.026520735, 0.7673636, 0.089263...   \n",
       "6   [-0.10903608, 0.5291753, 0.030806696, -0.39204...   \n",
       "7   [0.18595317, -0.056578077, 0.451056, 0.0625802...   \n",
       "8   [-0.38173732, -0.06489463, 1.0201044, 0.250125...   \n",
       "9   [0.23779905, -0.11628204, 0.6812859, -0.343407...   \n",
       "10  [-0.22223017, 0.6175762, 1.0857452, 0.01081747...   \n",
       "11  [0.17047112, -0.07413415, 0.9424215, -0.334402...   \n",
       "12  [0.031640965, -0.19801572, 0.5024514, 0.065041...   \n",
       "13  [0.36531875, 0.37158275, 0.48456764, -0.232610...   \n",
       "14  [0.30118367, 0.5062818, 0.58145624, 0.10106157...   \n",
       "15  [0.17070527, -0.22292602, 0.8498536, 0.0013588...   \n",
       "16  [-0.23170915, 0.3670838, 0.4376015, -0.5879583...   \n",
       "17  [-0.16830447, 0.07773729, 0.5605185, -0.296834...   \n",
       "18  [0.043046813, 0.39170945, 0.26555818, -0.63678...   \n",
       "19  [-0.33113652, -0.14651328, 0.5758766, -0.33886...   \n",
       "20  [0.10956125, -0.2147402, 0.39737117, 0.0227459...   \n",
       "21  [0.40800375, 0.006384422, 0.8019866, -0.236924...   \n",
       "22  [0.56484497, -0.751867, 0.5702715, 0.5117423, ...   \n",
       "23  [0.22978468, -0.4031109, 0.9669361, -0.0426368...   \n",
       "24  [-0.09648979, 0.055998657, 1.4428711, 0.317735...   \n",
       "25  [-0.45005807, 0.20697588, 0.52582383, 0.335635...   \n",
       "26  [-0.06273041, -0.26858333, 0.502122, 0.3729524...   \n",
       "27  [-0.20543465, 0.13389634, 0.76032037, 0.226678...   \n",
       "28  [0.32653117, 0.41252592, 1.0726038, -0.9254759...   \n",
       "29  [0.25647098, 0.19503014, 0.3793733, -0.1442564...   \n",
       "30  [0.01223959, 0.7328718, -0.1754158, 0.3445327,...   \n",
       "31  [0.115973815, 0.08454449, 0.667538, 0.10936332...   \n",
       "32  [0.17846943, 0.26125696, 0.9453256, 0.12821321...   \n",
       "33  [-0.09243641, 0.7118591, 0.34000805, -0.452339...   \n",
       "34  [-0.023184536, -0.6220377, 0.57217103, -0.5575...   \n",
       "35  [0.7407473, -0.31217045, 0.72125256, 0.1477113...   \n",
       "36  [0.21762559, -0.029377174, 1.1780553, 0.329879...   \n",
       "37  [0.47751266, 0.21097979, 0.7651377, -0.0166073...   \n",
       "38  [0.15831745, -0.12689763, 0.73806167, 0.061833...   \n",
       "39  [0.023847984, 0.103833035, 0.49484363, -0.5503...   \n",
       "40  [0.2534478, 0.1634743, 0.83604115, -0.2745389,...   \n",
       "41  [0.2833146, -0.016000703, -0.05473227, -0.0227...   \n",
       "42  [0.2342671, 0.5240219, 0.7462305, -0.2943097, ...   \n",
       "43  [-0.060757887, -0.06561157, 0.7962137, -0.2418...   \n",
       "44  [0.0524387, 0.3335148, 1.0886497, -0.23916577,...   \n",
       "45  [0.110550135, -0.41170192, 1.3172055, 0.057960...   \n",
       "\n",
       "                          User  Answer                        Keyword  \n",
       "0                         TA前川       1                  横浜をチャンスある街にする  \n",
       "1                         TA前川       2                    事業評価手法の開発運用  \n",
       "2                         TA前川       3                社会的インパクトを含む黒字事業  \n",
       "3                         TA前川       4                 アイデアサンドボックスの構築  \n",
       "4                          emo       1   市民、企業、行政が一体となり、高度な自治を実現している街  \n",
       "5                          emo       2              オープンイノベーションと子ども参加  \n",
       "6                          emo       3                   行政企業の共創による成果  \n",
       "7                          emo       4               子どもと企業の共創ワークショップ  \n",
       "8                           Yu       1                  コンヴィヴィアルな共生まち  \n",
       "9                           Yu       2                  横浜市の取り組みを発信する  \n",
       "10                          Yu       3  記事数、Webサイトへの訪問者数、記事によって感謝された数  \n",
       "11                          Yu       4        「循環経済とケア」をテーマとするイベントや事業  \n",
       "12         一般社団法人団地暮らしの共創　小柴健一       1              地域づくりをたすけあいで持続させる  \n",
       "13         一般社団法人団地暮らしの共創　小柴健一       2                  住民の声を共有し仕組み作り  \n",
       "14         一般社団法人団地暮らしの共創　小柴健一       3                地域の幸福度を調査データ化する  \n",
       "15         一般社団法人団地暮らしの共創　小柴健一       4                   様々な取り組みを統合する  \n",
       "16                       きんちゃん       1                 市内外に優れた雇用環境を提供  \n",
       "17                       きんちゃん       2                 オープンイノベーションの推進  \n",
       "18                       きんちゃん       3               オープンイノベーションの投資効果  \n",
       "19                       きんちゃん       4                オープンイノベーションの連携場  \n",
       "20  Circular Yokohama（ハーチ株式会社）       1                  創造的な街づくりを推進する  \n",
       "21  Circular Yokohama（ハーチ株式会社）       2               20代の価値観希望理想を発信する  \n",
       "22  Circular Yokohama（ハーチ株式会社）       3                 市民の意識調査と投票率の状況  \n",
       "23                        藤原徹平       1                古きものから新しきものへの発展  \n",
       "24                        藤原徹平       2                      新旧を繋ぐ篩として  \n",
       "25                        藤原徹平       3                   創造的なコミュニティ活動  \n",
       "26                        藤原徹平       4                継続的な学びの場：市役所低層部  \n",
       "27      NPO法人KUSC・神奈川大学サッカー部藤森       1              農福スポーツ連携でQOL維持のまち  \n",
       "28      NPO法人KUSC・神奈川大学サッカー部藤森       2                  新事業の創出支援を実施する  \n",
       "29      NPO法人KUSC・神奈川大学サッカー部藤森       3              竹山団地すすきの団地事業支援の実績  \n",
       "30      NPO法人KUSC・神奈川大学サッカー部藤森       4          竹山団地すすきの団地のふるさと納税取り組み  \n",
       "31            コミュニティデザイン・ラボ　町山       1                     愛を持って自立する街  \n",
       "32            コミュニティデザイン・ラボ　町山       2                    共創案件のブリッジ人材  \n",
       "33            コミュニティデザイン・ラボ　町山       3   自立事業としての案件成功数、経済的（雇用率等含む）な結果  \n",
       "34            コミュニティデザイン・ラボ　町山       4                  インパクトのある事業の紹介  \n",
       "35                   とつかリビングラボ       1                  京都の18区の特徴を楽しむ  \n",
       "36                   とつかリビングラボ       2           それぞれに特徴を活かしあえる情報発信基地  \n",
       "37                   とつかリビングラボ       3   Well-being指標で図る。市民総幸福量の創設など。  \n",
       "38                   とつかリビングラボ       4               人口減少社会でも明るいまちづくり  \n",
       "39                        ミッキー       1   企業と市民が共に助け合い地域の課題に立ち向かう先駆的な街  \n",
       "40                        ミッキー       2               自治体ニーズと企業シーズの触媒役  \n",
       "41                        ミッキー       3                実証実験数、寄付件数、寄付金額  \n",
       "42                        ミッキー       4                   中小企業診断士育成の研修  \n",
       "43                        いよいよ       1                エコロジーとテクノロジーの共生  \n",
       "44                        いよいよ       2                  地域貢献企業認定制度を作る  \n",
       "45                        いよいよ       3                    スキームが構築できた時  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_sentence_list = []\n",
    "fc_vector_list = []\n",
    "fc_user_list = []\n",
    "fc_ans_list =[]\n",
    "fc_keyword_list = []\n",
    "for user_i in range(len(opt_df)):\n",
    "    print(f\"Now calculating {str(user_i+1)} of {str(len(opt_df))}'s data...\")\n",
    "    user = opt_df[\"email\"][user_i]\n",
    "    for ans_i in range(ans_max):\n",
    "        fc_sentence = opt_df[f\"ans{str(ans_i+1)}\"][user_i]\n",
    "        if type(fc_sentence) == float:\n",
    "            fc_sentence = \"\"\n",
    "        else:\n",
    "            fc_sentence = fc_sentence.strip()\n",
    "            \n",
    "        if len(fc_sentence) >= 3:\n",
    "            fc_sentence_list.append(fc_sentence)\n",
    "            fc_vector_list.append(get_sentence_vector(fc_sentence))\n",
    "            fc_user_list.append(user)\n",
    "            fc_ans_list.append(ans_i+1)\n",
    "            \n",
    "            if len(fc_sentence) < 30:\n",
    "                fc_keyword_list.append(fc_sentence)\n",
    "            else:\n",
    "                fc_keyword = get_results_for_one_text(fc_sentence[:OUTPUT_TOKENS], PROMPT_TEXT_KADAI_CORP)\n",
    "                fc_keyword_list.append(fc_keyword[0].replace(\"・\", \"\").replace(\" \", \"\").replace(\"　\", \"\").replace(\"「\", \"\").replace(\"」\", \"\"))\n",
    "\n",
    "fc_all = pd.DataFrame({\n",
    "    'Sentence': fc_sentence_list, \n",
    "    'Vector': fc_vector_list, \n",
    "    'User': fc_user_list, \n",
    "    'Answer': fc_ans_list, \n",
    "    'Keyword': fc_keyword_list\n",
    "})\n",
    "\n",
    "\n",
    "user_all = fc_all[\"User\"].unique()\n",
    "answer_all = fc_all[\"Answer\"].unique()\n",
    "fc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8748f2da-ceab-4d16-959d-9cc0906ddb59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "8110\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "./htmls/fc_yoko_fcdl.html\n"
     ]
    }
   ],
   "source": [
    "# clustering_and_create_network(fc_all, embeddings_kuma_add, sentences_kuma_add, \"kumaadd\", 5, 25)\n",
    "clustering_and_create_network(fc_all, embeddings_yoko_desc, embeddings_kadai, embeddings_corp, embeddings_hojo_desc, sentences_yoko_desc_df, sentences_kadai_df, sentences_corp_df, sentences_hojo_desc_df, \"yoko\", 5, 25)\n",
    "# clustering_and_create_network(fc_all, embeddings_yoko_desc, sentences_yoko_desc, \"yoko_desc\", 5, 25)\n",
    "# clustering_and_create_network(fc_all, embeddings, sentences, \"all\", 5, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a7236ed-97f0-4d83-aaed-04f85f5e01a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    for ans in answer_all:\n",
    "        fc_df = fc_all[fc_all[\"Answer\"] == ans].reset_index(drop=True).copy()\n",
    "        clustering_and_create_network(fc_df, f\"ans{ans}\", 4, 25)\n",
    "\n",
    "if False:\n",
    "    for user in user_all:\n",
    "        fc_df = fc_all[fc_all[\"User\"] == user].reset_index(drop=True).copy()\n",
    "        clustering_and_create_network(fc_df, f\"kpi-user-{user}\", 4, 25)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
